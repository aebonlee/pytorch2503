{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8XHHT99-Slm"
   },
   "source": [
    "### Chapter 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트랜스포머 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaV6bKLx-Sls"
   },
   "source": [
    "> ## 학습 목표\n",
    "-   **트랜스포머 개념 및 동작 원리 이해**: 트랜스포머 아키텍처의 기본 개념과 작동 방식을 명확히 이해하고 특히, 주목(attention) 메커니즘과 인코더-디코더 구조의 역할을 설명할 수 있다.\n",
    "    \n",
    "-   **트랜스포머 구현 능력**: 트랜스포머 모델을 실제로 구현하고, 다양한 프레임워크(예: TensorFlow, PyTorch)를 활용하여 모델을 훈련시키고 평가할 수 있다.\n",
    "    \n",
    "-   **자연어 처리(NLP) 분야에서의 응용 사례 분석**: 트랜스포머 모델이 자연어 처리에서 어떻게 활용되는지 구체적인 사례를 통해 설명할 수 있으며, 예를 들어, 기계 번역, 텍스트 요약, 감정 분석 등 다양한 응용 분야를 제시할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 트랜스포머란 무엇인가요?\n",
    "\n",
    "- 트랜스포머는 자연어 처리(NLP) 분야에서 혁신을 일으킨 모델입니다. \n",
    "- 2017년 구글에서 발표한 논문 \"Attention is All You Need\"에서 처음 소개되었으며, 이후 다양한 응용 분야에서 뛰어난 성능을 보여주고 있습니다. \n",
    "- 트랜스포머는 주로 번역, 텍스트 생성, 감정 분석 등에서 사용됩니다.\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "- **자기 주의 메커니즘(Self-Attention):** 입력 시퀀스의 각 단어가 다른 단어들과 얼마나 관련이 있는지를 학습합니다.\n",
    "- **병렬 처리:** RNN과 달리 입력 데이터를 동시에 처리할 수 있어 학습 속도가 빠릅니다.\n",
    "- **스케일러빌리티:** 대규모 데이터와 모델에 잘 맞습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트랜스포머의 구성 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/14.1_transformer model.png\" width=\"450\" height=\"\" >\n",
    "<figcaption>그림 14.1 transformer model : 출처: Attention is All You Need [https://arxiv.org/pdf/1706.03762.pdf]</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트랜스포머는 크게 인코더(Encoder)와 디코더(Decoder)로 구성됩니다. 각 인코더와 디코더는 여러 개의 층으로 이루어져 있습니다.\n",
    "\n",
    "### ■ 인코더\n",
    "- **다중 헤드 자기 주의(Multi-Head Self-Attention):** 입력된 단어들 간의 관계를 파악합니다.\n",
    "- **피드 포워드 신경망(Feed Forward Neural Network):** 정보를 변환하여 다음 층으로 전달합니다.\n",
    "\n",
    "### ■ 디코더\n",
    "\n",
    "- **마스크드 자기 주의(Masked Self-Attention):** 디코더가 이전 단어들만을 참조하도록 합니다.\n",
    "- **인코더-디코더 주의(Encoder-Decoder Attention):** 인코더의 출력을 참고하여 다음 단어를 예측합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **멀티헤드 어텐션(Multi-Head Attention)**: 여러 어텐션 메커니즘을 동시에 활용하기\n",
    "- **포지셔널 인코딩(Positional Encoding)**: 순서를 고려하기 위해 입력 데이터에 추가하는 정보\n",
    "- **피드 포워드 신경망(Feed Forward Neural Network)**: 각 위치에서의 처리를 담당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 동작 방식\n",
    "\n",
    "1. **쿼리(Query), 키(Key), 값(Value) 벡터 생성:** 입력 단어를 세 가지 벡터로 변환합니다.\n",
    "2. **유사도 계산:** 쿼리와 키의 내적을 통해 유사도를 계산합니다.\n",
    "3. **가중 합산:** 유사도를 기반으로 값 벡터를 가중 합산하여 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # 입력 텍스트를 임베딩된 벡터로 변환하는 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        # Transformer 모델 정의\n",
    "        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers, batch_first=True)\n",
    "        \n",
    "        # Transformer의 출력값을 원래의 입력 차원으로 변환하는 선형 레이어\n",
    "        self.fc_out = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # 입력 소스(src)와 타겟(tgt)을 임베딩하여 고차원 벡터로 변환\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        \n",
    "        # 트랜스포머 모델에 입력값 전달\n",
    "        output = self.transformer(src, tgt)\n",
    "        \n",
    "        # 트랜스포머의 출력을 원래의 입력 차원으로 변환\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# 파라미터 설정\n",
    "input_dim = 1000  # 어휘 수\n",
    "embed_dim = 64    # 임베딩 차원\n",
    "num_heads = 8     # 멀티헤드 어텐션의 헤드 수\n",
    "num_layers = 6    # 트랜스포머 레이어 수\n",
    "\n",
    "model = TransformerModel(input_dim, embed_dim, num_heads, num_layers)\n",
    "\n",
    "# 예제 입력 데이터 (배치 크기 2, 시퀀스 길이 10)\n",
    "src = torch.randint(0, input_dim, (2, 10))\n",
    "tgt = torch.randint(0, input_dim, (2, 10))\n",
    "\n",
    "# 모델의 출력 확인\n",
    "output = model(src, tgt)\n",
    "print(output.shape)  # (batch_size, seq_length, input_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **임베딩 레이어 (nn.Embedding)** :\n",
    "\n",
    "입력 텍스트(단어 또는 토큰)를 고정된 차원의 임베딩 벡터로 변환합니다.\n",
    "input_dim은 단어 집합의 크기(즉, 단어 수), embed_dim은 각 단어를 임베딩할 벡터의 차원입니다.\n",
    "\n",
    "- **트랜스포머 레이어 (nn.Transformer)** :\n",
    "\n",
    "트랜스포머 모델의 핵심 부분입니다.\n",
    "embed_dim은 각 입력 벡터의 차원입니다.\n",
    "num_heads는 어텐션 메커니즘에서 사용하는 헤드 수로, 여러 부분을 동시에 살펴보는 데 도움이 됩니다.\n",
    "num_layers는 트랜스포머 네트워크의 레이어 수를 나타냅니다. 더 많은 레이어는 더 복잡한 표현을 배울 수 있습니다.\n",
    "\n",
    "- **선형 레이어 (nn.Linear)** :\n",
    "\n",
    "트랜스포머 출력 벡터(차원 embed_dim)를 다시 원래의 입력 차원(input_dim)으로 변환하는 역할을 합니다.\n",
    "이는 주로 예측을 위해 사용되며, 예를 들어 언어 모델링이나 분류 작업에서 출력 결과를 생성하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 파이토치로 배우는 트랜스포머: 자연어 처리의 혁신\n",
    "\n",
    "**1. 트랜스포머: 순환 신경망을 넘어서**\n",
    "\n",
    "- **RNN의 한계:** RNN은 시퀀스 데이터를 처리하는 데 유용하지만, 장기 의존성을 학습하는 데 어려움을 겪습니다. 즉, 문장에서 멀리 떨어져 있는 단어들 간의 관계를 파악하는 데 한계가 있습니다.\n",
    "- **트랜스포머의 등장:** 트랜스포머는 RNN의 이러한 한계를 극복하기 위해 등장했습니다. 트랜스포머는 **주의 메커니즘**을 사용하여 입력 시퀀스의 모든 단어 간의 관계를 동시에 학습합니다. 이를 통해 장기 의존성을 효과적으로 파악할 수 있습니다.\n",
    "\n",
    "**2. 트랜스포머의 핵심: 어텐션 메커니즘**\n",
    "\n",
    "트랜스포머의 핵심은 **어텐션 메커니즘**입니다. 어텐션은 입력 시퀀스의 어떤 부분에 집중해야 하는지 결정하는 메커니즘입니다. \n",
    "\n",
    "\"나는 오늘 **학교**에 간다\"라는 문장에서 \"학교\"라는 단어에 집중해야 한다면, 어텐션은 \"학교\"에 높은 값을 부여합니다.\n",
    "\n",
    "**3. 파이토치로 트랜스포머 구현하기**\n",
    "\n",
    "- 파이토치를 사용하여 간단한 트랜스포머 모델을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, batch_first=True), num_encoder_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead, batch_first=True), num_decoder_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # Transformer expects (batch, seq, features) when batch_first=True\n",
    "        encoder_output = self.encoder(src, src_key_padding_mask=src_mask)\n",
    "        decoder_output = self.decoder(tgt, encoder_output, tgt_key_padding_mask=tgt_mask)\n",
    "        return decoder_output\n",
    "\n",
    "# 모델의 하이퍼파라미터 설정\n",
    "d_model = 512            # 모델의 임베딩 차원 (각 단어 벡터의 차원)\n",
    "nhead = 8                # 멀티헤드 어텐션에서의 헤드 수\n",
    "num_encoder_layers = 6   # 인코더의 레이어 수\n",
    "num_decoder_layers = 6   # 디코더의 레이어 수\n",
    "\n",
    "# Transformer 모델 인스턴스 생성\n",
    "model = Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "# 입력 데이터 (예시)\n",
    "src = torch.randn(32, 10, 512)  # (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "tgt = torch.randn(32, 10, 512)  # (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "\n",
    "# 마스크 생성 예시 (배치 크기, 시퀀스 길이), 마스크는 패딩된 부분을 마스킹용\n",
    "src_mask = (src[:, :, 0] == 0)  # 예시로 첫 번째 차원이 0인 부분을 마스킹\n",
    "tgt_mask = (tgt[:, :, 0] == 0)  # 예시로 첫 번째 차원이 0인 부분을 마스킹\n",
    "\n",
    "# 모델 실행\n",
    "output = model(src, tgt, src_mask, tgt_mask)\n",
    "print(output.shape)  # (배치 크기, 시퀀스 길이, 임베딩 차원)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 파이토치의 `nn.Transformer` 모듈을 사용하여 간단한 트랜스포머 모델을 구현한 것입니다. `d_model`, `nhead`, `num_encoder_layers`, `num_decoder_layers`는 모델의 하이퍼파라미터입니다. `forward` 함수는 인코더와 디코더를 통해 입력 시퀀스를 처리하고 출력을 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트랜스포머 응용 사례\n",
    "- **번역:** 구글 번역, 딥엘(DeepL) 등에서 사용됩니다.\n",
    "- **챗봇:** 트랜스포머는 대화 맥락을 이해하고 적절한 응답을 생성하는 데 사용됩니다.\n",
    "- **텍스트 생성:** GPT 시리즈가 대표적입니다.\n",
    "- **텍스트 요약:** 트랜스포머는 긴 텍스트를 요약하여 핵심 정보를 추출하는 데 사용됩니다.\n",
    "- **음성 인식:** 음성을 텍스트로 변환하는 데 사용됩니다.\n",
    "- **이미지 처리:** 비전 트랜스포머(Vision Transformer)로 이미지 분류 등에 응용됩니다.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 **간단한 예제: 번역 모델 만들기**\n",
    "\n",
    "이제 파이토치를 사용하여 간단한 트랜스포머 기반 번역 모델을 만들어보겠습니다.\n",
    "\n",
    "- 파이토치 설치\n",
    "- 데이터셋 (예: 영어-한국어 문장 쌍)\n",
    "\n",
    "### **단계별 과정**\n",
    "\n",
    "1. **데이터 준비:** 문장을 토큰화하고, 정수 인코딩을 수행합니다.\n",
    "2. **모델 정의:** 트랜스포머 인코더와 디코더를 정의합니다.\n",
    "3. **학습:** 손실 함수를 정의하고, 옵티마이저를 사용하여 모델을 학습시킵니다.\n",
    "4. **평가:** 테스트 데이터를 사용하여 모델의 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 9.4065\n",
      "Epoch [2/10], Loss: 9.0561\n",
      "Epoch [3/10], Loss: 8.8186\n",
      "Epoch [4/10], Loss: 8.6396\n",
      "Epoch [5/10], Loss: 8.4242\n",
      "Epoch [6/10], Loss: 8.1980\n",
      "Epoch [7/10], Loss: 8.0004\n",
      "Epoch [8/10], Loss: 7.8312\n",
      "Epoch [9/10], Loss: 7.7114\n",
      "Epoch [10/10], Loss: 7.5606\n",
      "Predicted tokens: tensor([8583, 8583, 8583, 8583, 3938, 8583, 8583, 8583, 2938])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Transformer\n",
    "\n",
    "# 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # 트랜스포머 정의 (batch_first=True 설정으로 경고 제거)\n",
    "        self.transformer = Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, batch_first=True)\n",
    "        # 소스 시퀀스를 임베드하는 임베딩 레이어\n",
    "        self.src_embedding = nn.Embedding(src_vocab, d_model)\n",
    "        # 타겟 시퀀스를 임베드하는 임베딩 레이어\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab, d_model)\n",
    "        # 트랜스포머 출력 변환을 위한 선형 레이어\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # 소스 및 타겟 시퀀스를 임베딩\n",
    "        src_emb = self.src_embedding(src)\n",
    "        tgt_emb = self.tgt_embedding(tgt)\n",
    "        # 트랜스포머를 통해 출력 계산\n",
    "        output = self.transformer(src_emb, tgt_emb)\n",
    "        # 출력 변환\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# 모델 초기화\n",
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size)\n",
    "\n",
    "# 데이터 준비 (예시)\n",
    "src_sentence = torch.randint(1, src_vocab_size, (32, 10))  \n",
    "# (배치 크기, 시퀀스 길이)\n",
    "tgt_sentence = torch.randint(1, tgt_vocab_size, (32, 20))  \n",
    "# (배치 크기, 시퀀스 길이)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 패딩 토큰에 대한 손실 무시\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 학습 루프 정의\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # 모델의 예측\n",
    "    output = model(src_sentence, tgt_sentence[:, :-1])\n",
    "    # 손실 계산\n",
    "    loss = criterion(output.view(-1, tgt_vocab_size), tgt_sentence[:, 1:].contiguous().view(-1))\n",
    "    # 역전파 및 옵티마이저 스텝\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 테스트\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 단일 테스트 샘플 준비 (소스 시퀀스)\n",
    "    test_src_sentence = torch.randint(1, src_vocab_size, (1, 10))\n",
    "    # 모델을 사용하여 예측 시퀀스 생성\n",
    "    test_tgt_sentence = model(test_src_sentence, test_src_sentence[:, :-1])\n",
    "    # 가장 높은 확률의 토큰 선택\n",
    "    predicted_tokens = test_tgt_sentence.argmax(dim=-1)[0]\n",
    "\n",
    "print(\"Predicted tokens:\", predicted_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드는 기본적인 트랜스포머 모델을 정의하는 예시입니다. \n",
    "- 실제로는 데이터 전처리, 학습 루프, 최적화 등이 추가로 필요하지만, 기본 구조를 이해하는 데 도움이 될 것입니다.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 자연어 생성(Language Generation)**\n",
    "\n",
    "### **이론적 배경**\n",
    "\n",
    "자연어 생성은 주어진 입력에 따라 자연스러운 텍스트를 생성하는 작업입니다. 트랜스포머 기반 모델인 GPT(Generative Pre-trained Transformer)는 이 분야에서 뛰어난 성능을 보여줍니다. GPT는 디코더만으로 구성된 트랜스포머로, 텍스트 생성에 최적화되어 있습니다.\n",
    "\n",
    "### **간단한 챗봇 만들기**\n",
    "\n",
    "간단한 챗봇을 만들어 사용자의 입력에 대해 응답을 생성해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"800\" height=\"\" src=\"./image/AnacondaPrompt.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conda install pytorch torchvision torchaudio -c pytorch**\n",
    "\n",
    "**conda install -c conda-forge transformers**\n",
    "\n",
    "**pip install 명령어를 사용하는 대신 Conda 환경에서 conda install로 설치하는 것을 권장합니다.**\n",
    "\n",
    "**pip uninstall torch transformers -y**\n",
    "\n",
    "**pip install torch transformers**\n",
    "\n",
    "**종종 패키지 설치가 부분적으로 잘못될 경우가 있습니다. 패키지를 삭제한 후 재설치해 보세요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%pip install torch transformers`와 `!pip install torch transformers`는 모두 파이썬 환경에서 패키지를 설치하는 명령어이지만, 사용하는 컨텍스트와 방식에 따라 차이가 있습니다.\n",
    "\n",
    "1.  **% 명령어 (매직 커맨드)**:\n",
    "    \n",
    "    -   `%pip`는 IPython이나 Jupyter Notebook에서 사용하는 매직 커맨드입니다. 이것은 IPython의 내부 메커니즘에 통합되어 있으며, 현재의 Python 환경에 직접적으로 영향을 미칩니다. 예를 들어, 패키지를 설치한 후, 동일한 셀에서 그 패키지를 즉시 사용할 수 있습니다.\n",
    "    -   `%pip install`을 사용할 경우, 설치가 완료된 후, 같은 셀 내에서 즉시 그 패키지를 사용할 수 있는 장점이 있습니다.\n",
    "    \n",
    "2.  **! 명령어 (쉘 커맨드)**:\n",
    "    \n",
    "    -   `!pip`는 일반적인 쉘 명령어를 실행하는 방식입니다. Jupyter Notebook에서 `!`로 시작하는 명령은 운영 체제의 쉘에서 실행됩니다. 이는 보통 스크립트나 셸 명령어를 실행할 때 사용됩니다.\n",
    "    -   `!pip install`을 사용할 경우, 패키지를 설치한 뒤 같은 셀에서 그 패키지를 사용할 수 없을 수 있습니다. 설치가 완료된 후에는 커널을 다시 시작하거나 다른 셀에서 패키지를 임포트해야 할 수도 있습니다.\n",
    "    \n",
    "\n",
    "요약하자면, `%pip`는 Jupyter 환경에 더 적합한 방식이고, `!pip`는 일반적인 쉘 명령어 실행 방식입니다. 어느 쪽을 사용하든 패키지 설치는 가능하지만, `%pip`가 설치 후 즉시 사용하기에 더 편리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 환경 설정\n",
    "먼저 필요한 라이브러리를 설치하고 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchtext in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchtext) (4.66.5)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torchtext) (1.26.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->torchtext) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->torchtext) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->torchtext) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "%pip install torch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Hello, how are you?\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고 자료\n",
    "\n",
    "-   **PyTorch 공식 문서**\n",
    "    \n",
    "    -   [PyTorch](https://pytorch.org/)\n",
    "    -   [nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n",
    "    \n",
    "-   **튜토리얼 및 가이드**\n",
    "    \n",
    "    -   [PyTorch 튜토리얼 - 번역기 구현](https://tutorials.pytorch.kr/beginner/translation_transformer.html)\n",
    "    -   [torchtext를 활용한 NLP 모델 구축](https://pytorch.org/text/stable/index.html)\n",
    "    \n",
    "-   **트랜스포머 논문**\n",
    "    \n",
    "    -   Vaswani et al., \"Attention Is All You Need\", 2017. [논문 원문](https://arxiv.org/abs/1706.03762)\n",
    "    \n",
    "\n",
    "___\n",
    "\n",
    "위의 코드는 간단한 예제이며, 실제로는 더 큰 데이터셋과 추가적인 전처리가 필요합니다. \n",
    "\n",
    "또한, 학습을 위한 하이퍼파라미터 튜닝과 모델 최적화가 필요할 수 있습니다. 하지만 이 예제를 통해 파이토치로 트랜스포머 기반의 챗봇을 구현하는 기본적인 흐름을 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = 'gpt2'  # 또는 'gpt2-medium', 'gpt2-large'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPU 사용 가능할 경우 GPU로 모델 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(text):\n",
    "    return tokenizer.encode(text, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, GPT 챗봇입니다. 무엇을 도와드릴까요?\n",
      "\n",
      "안녕하세요, GPT 챗봇입니다. 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "# 응답 생성 함수\n",
    "def generate_response(input_text, max_new_tokens=60):\n",
    "    model.eval()\n",
    "    input_ids = encode_input(input_text)\n",
    "    \n",
    "    # 응답 생성\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 테스트\n",
    "user_input = \"안녕하세요, GPT 챗봇입니다. 무엇을 도와드릴까요?\"\n",
    "response = generate_response(user_input)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇과 대화하려면 메시지를 입력하세요. 종료하려면 '그만'을 입력하세요.\n",
      "챗봇: 안녕안녕안녕안녕안녕안녕안녕안녕안녕안녕안녕\n",
      "챗봇: 대화를 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "def chat():\n",
    "    print(\"챗봇과 대화하려면 메시지를 입력하세요. 종료하려면 '그만'을 입력하세요.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"사용자: \")\n",
    "        if user_input.lower() == '그만':\n",
    "            print(\"챗봇: 대화를 종료합니다.\")\n",
    "            break\n",
    "        \n",
    "        response = generate_response(user_input)\n",
    "        print(f\"챗봇: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`트랜스포머 모델 챗봇만들기 예제`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 구성 요소:\n",
    "\n",
    "1.  **데이터 준비**:\n",
    "    \n",
    "    -   간단한 샘플 대화(`Q&A`) 데이터셋으로 `질문-답변 쌍`을 정의.\n",
    "    -   토크나이저로 단어를 ID로 변환.\n",
    "2.  **모델 정의**:\n",
    "    \n",
    "    -   PyTorch의 `Transformer` 기반 구조.\n",
    "    -   임베딩, 트랜스포머, 출력 레이어로 구성.\n",
    "3.  **훈련 루프**:\n",
    "    \n",
    "    -   주어진 데이터셋을 기반으로 모델을 학습.\n",
    "4.  **챗봇 인터페이스**:\n",
    "    \n",
    "    -   사용자가 질문을 입력하면, 모델이 가장 적합한 응답을 생성.\n",
    "\n",
    "※ 트랜스포머 챗봇의 기초적인 예제로, 학습 데이터를 확장하거나, 실제 데이터셋(예: Cornell Movie Dialogs)을 사용하면 더 적합한 응답을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇 학습을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300, Loss: 0.0033\n",
      "Epoch 100/300, Loss: 0.0012\n",
      "Epoch 150/300, Loss: 0.0007\n",
      "Epoch 200/300, Loss: 0.0004\n",
      "Epoch 250/300, Loss: 0.0002\n",
      "Epoch 300/300, Loss: 0.0002\n",
      "학습이 완료되었습니다!\n",
      "\n",
      "Chatbot is ready! Type '그만' to exit.\n",
      "Chatbot: 죄송해요, 적절한 답변을 찾지 못했어요.\n",
      "Chatbot: 클라우드에 살아요.\n",
      "Chatbot: 안녕히 가세요!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋 준비\n",
    "def build_dataset():\n",
    "    # 한국어 질문-답변 쌍 데이터\n",
    "    pairs = [\n",
    "        (\"안녕하세요\", \"안녕하세요\"),\n",
    "        (\"오늘 기분 어때요?\", \"좋아요\"),\n",
    "        (\"이름이 뭐예요?\", \"저는 챗봇이에요\"),\n",
    "        (\"어디 살아요?\", \"클라우드에 살아요\"),\n",
    "        (\"잘가요\", \"안녕히 가세요\"),\n",
    "        (\"무엇을 도와드릴까요?\", \"아무거나 물어보세요\"),\n",
    "        (\"오늘 날씨 어때요?\", \"모르겠어요\"),\n",
    "        (\"당신은 몇 살이에요?\", \"저는 나이가 없어요\"),\n",
    "        (\"취미가 뭐예요?\", \"취미는 없어요\"),\n",
    "        (\"좋아하는 영화가 뭐예요?\", \"영화는 잘 몰라요\")\n",
    "    ]\n",
    "    # 단어를 숫자 ID로 변환하기 위한 간단한 토크나이저\n",
    "    word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "    idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "    idx = 4\n",
    "    for pair in pairs:\n",
    "        for sentence in pair:\n",
    "            for word in sentence.split():\n",
    "                if word not in word2idx:\n",
    "                    word2idx[word] = idx\n",
    "                    idx2word[idx] = word\n",
    "                    idx += 1\n",
    "    return pairs, word2idx, idx2word\n",
    "\n",
    "# 텐서로 변환 + 토크나이저 적용\n",
    "def tokenize_and_pad(sentence, word2idx, eos=True):\n",
    "    tokens = [word2idx.get(word, word2idx[\"<unk>\"]) for word in sentence.split()]\n",
    "    if eos:\n",
    "        tokens.append(word2idx[\"<eos>\"])\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "## 모델 정의\n",
    "class ChatbotTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, num_heads=2, num_layers=2, ff_dim=128, max_len=50):\n",
    "        super(ChatbotTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(self.create_positional_encoding(embed_size, max_len))\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def create_positional_encoding(self, embed_size, max_len):\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-np.log(10000.0) / embed_size))\n",
    "        pos_enc = torch.zeros(max_len, embed_size)\n",
    "        pos_enc[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(pos * div_term)\n",
    "        return pos_enc.unsqueeze(0)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        src_emb = src_emb.permute(1, 0, 2)\n",
    "        tgt_emb = tgt_emb.permute(1, 0, 2)\n",
    "        transformer_out = self.transformer(src_emb, tgt_emb)\n",
    "        output = self.fc_out(transformer_out.permute(1, 0, 2))\n",
    "        return output\n",
    "\n",
    "## 학습 루프\n",
    "def train(pairs, word2idx, idx2word, model, optimizer, criterion, num_epochs=300, batch_size=2):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        random.shuffle(pairs)\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch_pairs = pairs[i:i + batch_size]\n",
    "            src_seq = []\n",
    "            tgt_seq = []\n",
    "            for pair in batch_pairs:\n",
    "                src_seq.append(tokenize_and_pad(pair[0], word2idx))\n",
    "                tgt_seq.append(tokenize_and_pad(pair[1], word2idx, eos=True))\n",
    "            src_padded = pad_sequence(src_seq, batch_first=True, padding_value=0)\n",
    "            tgt_input = pad_sequence([seq[:-1] for seq in tgt_seq], batch_first=True, padding_value=0)\n",
    "            tgt_target = pad_sequence([seq[1:] for seq in tgt_seq], batch_first=True, padding_value=0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_padded, tgt_input)\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_target.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(pairs):.4f}\")\n",
    "\n",
    "## 예측\n",
    "def translate(model, input_sentence, word2idx, idx2word, max_len=20):\n",
    "    model.eval()\n",
    "    \n",
    "    src_tokens = [word2idx.get(word, word2idx[\"<unk>\"]) for word in input_sentence.split()]\n",
    "    src = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tgt = torch.tensor([word2idx[\"<sos>\"]], dtype=torch.long).unsqueeze(0)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            output = model(src, tgt)\n",
    "            next_token = output[:, -1, :].argmax(dim=-1).item()\n",
    "            if next_token == word2idx[\"<eos>\"]:\n",
    "                break\n",
    "            tgt = torch.cat([tgt, torch.tensor([[next_token]], dtype=torch.long)], dim=1)\n",
    "        \n",
    "        response_tokens = [idx2word[token.item()] for token in tgt[0][1:] \n",
    "                        if token.item() in idx2word \n",
    "                        and idx2word[token.item()] not in [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"]]\n",
    "        \n",
    "        if not response_tokens:\n",
    "            return \"죄송해요, 적절한 답변을 찾지 못했어요.\"\n",
    "            \n",
    "        return \" \".join(response_tokens)\n",
    "\n",
    "## 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    # 데이터 준비\n",
    "    pairs, word2idx, idx2word = build_dataset()\n",
    "    vocab_size = len(word2idx)\n",
    "\n",
    "    # 모델 초기화\n",
    "    model = ChatbotTransformer(vocab_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # 학습\n",
    "    print(\"챗봇 학습을 시작합니다...\")\n",
    "    train(pairs, word2idx, idx2word, model, optimizer, criterion)\n",
    "    print(\"학습이 완료되었습니다!\")\n",
    "\n",
    "    # 챗봇 테스트\n",
    "    print(\"\\nChatbot is ready! Type '그만' to exit.\")\n",
    "    \n",
    "    # 미리 정의된 응답 사전\n",
    "    predefined_responses = {\n",
    "        \"안녕하세요\": \"안녕하세요! 반갑습니다.\",\n",
    "        \"어디 살아요?\": \"클라우드에 살아요.\",\n",
    "        \"이름이 뭐예요?\": \"저는 챗봇이에요.\",\n",
    "        \"오늘 기분 어때요?\": \"좋아요!\",\n",
    "        \"잘가요\": \"안녕히 가세요.\",\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input == \"그만\":\n",
    "            print(\"Chatbot: 안녕히 가세요!\")\n",
    "            break\n",
    "            \n",
    "        if user_input in predefined_responses:\n",
    "            print(f\"Chatbot: {predefined_responses[user_input]}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = translate(model, user_input, word2idx, idx2word)\n",
    "            print(f\"Chatbot: {response}\")\n",
    "        except Exception as e:\n",
    "            print(\"Chatbot: 죄송해요, 답변을 생성하는 데 문제가 있었어요.\")\n",
    "            continue\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
