{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 학습목표\n",
    "\n",
    "- PyTorch의 주요 특성과 기능 (동적 계산 그래프, 자동 미분, 텐서 연산 등)을 이해하고, PyTorch의 사용 사례를 이해할 수 있다. \n",
    "- Python, Anaconda, Visual Studio Code, Google Colab 등을 설치하고 설정하여 PyTorch 개발 환경을 구축할 수 있다. \n",
    "- GPU의 역할과 중요성을 이해하고, GPU를 사용하여 PyTorch 연산을 실행하는 방법을 학습합니다. 또한, GPU가 없는 경우의 오류 처리할 수 있다.\n",
    "- 텐서 생성 및 조작, 모델 정의 및 학습, CPU와 GPU 간의 텐서 이동 등을 포함한 PyTorch의 기본 사용법을 이해할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이토치(PyTorch)는 Facebook(Meta AI)의 AI 연구팀에서 개발한 오픈소스 머신러닝 라이브러리, Python 기반의 딥러닝 프레임워크입니다. \n",
    "- PyTorch는 2016년에 처음 발표되었고, Facebook AI Research(FAIR) 팀이 주도하여 개발하였습니다. \n",
    "- Meta는 PyTorch를 사용하여 컴퓨터 비전, 자연어 처리(NLP), 추천 시스템 등 다양한 분야에서 연구하고 있습니다.\n",
    "- Microsoft, Google, Tesla, Uber, NVIDIA 등 기업들이 널리 사용하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활용 분야\n",
    "\n",
    "- 컴퓨터 비전, 자연어 처리, 강화학습, 음성 인식, 생성 모델, 딥러닝 연구와 개발에 사용됩니다.\n",
    "- 자율 주행, 수요 예측 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PyTorch 특징`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 사용 편의성, 유연성, 딥러닝 모델의 빠른 개발 속도가 특징인 오픈 소스 딥러닝 프레임워크입니다.\n",
    "2. **파이토치는 동적 계산 그래프(dynamic computation graph)를 지원**합니다.\n",
    "   모델의 구조를 학습 중에 동적으로 변경할 수 있어, 디버깅이나 실험을 더 유연하게 수행할 수 있게 합니다. 이를 통해 연구자와 개발자가 모델의 구조를 학습 과정 중에도 변경할 수 있습니다. 이 기법은 모델을 실험하고 디버깅하는 데 매우 유용합니다.\n",
    "\n",
    "3. **자동 미분(Autograd):** 파이토치는 자동 미분 기능을 제공하여, 역전파(backpropagation) 과정에서 기울기를 자동으로 계산합니다. \n",
    "   이 기능은 신경망 학습에서 필수적인 역할을 하며, 복잡한 미분을 수동으로 계산할 필요가 없게 만듭니다.\n",
    "  torch.autograd 패키지를 통해 계산 그래프를 동적으로 구성하고, 역전파(Backpropagation) 알고리즘을 통해 파라미터를 업데이트할 수 있습니다.\n",
    "  역전파(backpropagation) 과정에서 기울기를 자동으로 계산해 주는 기능으로, 신경망 학습에서 필수적입니다.\n",
    "  4. __텐서(Tensor):__ 파이토치는 텐서 연산을 기본으로 하는 라이브러리로, NumPy와 유사합니다. \n",
    "  텐서는 데이터를 나타내는 다차원 배열로, CPU 및 GPU에서 연산을 효율적으로 처리할 수 있습니다.\n",
    "  텐서를 이용해 행렬 연산이나 벡터 연산 등을 수행할 수 있습니다.\n",
    "5. **GPU 가속:** 파이토치는 CUDA를 지원하여, GPU에서 연산을 수행하여 딥러닝 모델 학습을 가속화할 수 있습니다. \n",
    "   이는 대규모 데이터셋과 복잡한 모델을 다룰 때 매우 유리합니다.\n",
    "   torch.cuda 모듈을 사용하면 CUDA를 통한 GPU 연산을 손쉽게 할 수 있습니다.\n",
    "6. **모델과 학습:** 파이토치는 딥러닝 모델을 쉽게 정의할 수 있게 도와주는 nn.Module 클래스를 제공합니다. 모델을 정의할 때 forward 메서드를 통해 순전파 과정을 정의합니다. 모델 학습을 위한 torch.optim 모듈을 제공하여, 경사하강법(Gradient Descent) 및 다양한 최적화 알고리즘(SGD, Adam 등)을 지원합니다.\n",
    "  torch.nn 모듈은 신경망 레이어, 손실 함수, 최적화 방법 등을 포함하고 있어, 복잡한 딥러닝 모델을 손쉽게 구현할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GPU 사용 여부 체크하기**\n",
    "\n",
    "- 텐서간의 연산을 수행할 때, 기본적으로 두 텐서가 같은 장치에 있어야 한다.\n",
    "- 따라서 가능하면, 연산을 수행하는 텐서들을 모두 GPU에 올린 뒤에 연산을 수행한다.\n",
    "- GPU 사용여부 확인 코드 torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Python 설치:**\n",
    "\n",
    "Python 3.x 다운로드 페이지에서 본인 운영체제에 맞는 Python 설치 파일을 다운로드하고 설치합니다.\n",
    "\n",
    "설치 후 python --version 명령으로 설치가 제대로 되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500\" height=\"\" src=\"./image/01.python설치.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Anaconda 설치:**\n",
    "\n",
    "- Anaconda Individual Edition 다운로드 페이지에서 Anaconda를 다운로드하고 설치합니다.\n",
    "\n",
    "- Anaconda는 필요한 패키지를 쉽게 설치하고 관리할 수 있습니다.\n",
    "\n",
    "- 설치 후 conda --version 명령으로 설치가 제대로 되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500\" height=\"\" src=\"./image/01.anaconda설치.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PyTorch 설치:**\n",
    "\n",
    "- Anaconda 프롬프트를 엽니다.\n",
    "\n",
    "1. **GPU가 없는 경우** :\n",
    "**conda install pytorch cpuonly -c pytorch**\n",
    "\n",
    "2. **CUDA를 지원하는 GPU가 있는 경우** : \n",
    "**conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"700\" height=\"\" src=\"./image/01.cpu.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"700\" height=\"\" src=\"./image/01.gpu.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PyTorch 설치 확인:**\n",
    "\n",
    "- Python 인터프리터를 열고 다음 코드를 실행하여 PyTorch가 제대로 설치되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ 기존에 설치된 파이썬, 아나콘다의 버전이 상이하거나 패키지가 충돌되어 에러가 발생할 때 참고하세요.\n",
    "\n",
    "conda install conda=24.9.2\n",
    "\n",
    "conda create -n pytorch python=3.12.7\n",
    "\n",
    "conda activate pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 사용 불가능\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 사용 가능\")\n",
    "    print(\"사용 가능한 GPU 개수:\", torch.cuda.device_count())\n",
    "    print(\"현재 사용 중인 GPU:\", torch.cuda.current_device())\n",
    "    print(\"GPU 이름:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"GPU 사용 불가능\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mis_cuda)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# GPU로 옮기기\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mis_cuda) \u001b[38;5;66;03m# print(x.is_cuda)는 False를 출력합니다.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;66;03m# CPU로 옮기기\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# GPU가 없다면 에러\n",
    "import torch\n",
    "\n",
    "sample = [[2, 4], [6, 8]]\n",
    "\n",
    "x = torch.tensor(sample)  # torch.tensor(sample)을 호출하여 텐서를 생성합니다.\n",
    "print(x.is_cuda)\n",
    "\n",
    "# GPU로 옮기기\n",
    "x = x.cuda()\n",
    "print(x.is_cuda) # print(x.is_cuda)는 False를 출력합니다.\n",
    "\n",
    "x = x.cpu() # CPU로 옮기기\n",
    "print(x.is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결괏값 해설\n",
    "\n",
    "   - 결괏값 1 : 처음 생성된 텐서 x는 GPU가 아닌 CPU에 할당되어 있기 때문에 x.is_cuda는 False를 출력합니다.\n",
    "\n",
    "   - 결괏값 2 : 텐서를 GPU로 옮기면 x.is_cuda는 True를 출력합니다.\n",
    "\n",
    "   - 결괏값 3 : 다시 CPU로 옮기면 x.is_cuda는 False를 출력합니다.\n",
    "\n",
    "※ 따라서 GPU가 없는 경우 다음과 같은 코드 블록을 통해 x.cuda() 호출 전에 GPU가 사용 가능한지 확인하고, 그렇지 않으면 경고 메시지를 출력하는 방법을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "CUDA를 사용할 수 없습니다. GPU가 없는 시스템입니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sample = [[2, 4], [6, 8]]\n",
    "\n",
    "x = torch.tensor(sample)\n",
    "\n",
    "print(x.is_cuda)  # Output: False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    print(x.is_cuda)  # Output: True\n",
    "\n",
    "    x = x.cpu()\n",
    "    print(x.is_cuda)  # Output: False\n",
    "else:\n",
    "    print(\"CUDA를 사용할 수 없습니다. GPU가 없는 시스템입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# device 확인\n",
    "import torch\n",
    "\n",
    "tensor = torch.rand(3, 4)\n",
    "print(f\"Device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 텐서가 GPU에 있는지 여부: False\n",
      "Torch 텐서가 GPU에 있는지 여부: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 샘플 데이터\n",
    "sample = [[2, 4], [6, 8]]\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # GPU를 사용\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPU를 사용\n",
    "\n",
    "# 텐서를 생성하고, GPU로 옮기기\n",
    "x = torch.tensor(sample).to(device)\n",
    "print(f\"Torch 텐서가 GPU에 있는지 여부: {x.is_cuda}\")  # True여야 GPU에서 연산 중\n",
    "\n",
    "# CUDA 장치 이름 출력 (확인용)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"사용 중인 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 텐서를 다시 CPU로 옮기기\n",
    "x = x.cpu()\n",
    "print(f\"Torch 텐서가 GPU에 있는지 여부: {x.is_cuda}\")  \n",
    "# False여야 CPU에서 연산 중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# tensor가 GPU에 있는지 확인\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 만약 GPU가 아닌 'cuda:0' 장치에 텐서를 명시적으로 옮기고 싶다면 아래처럼 작성\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 다시 디바이스 출력\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice (명시적 이동 후): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 샘플 데이터\n",
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # GPU를 사용\n",
    "    print(f\"사용 가능한 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPU를 사용\n",
    "\n",
    "# 텐서를 GPU로 옮기기\n",
    "tensor = tensor.to(device)\n",
    "\n",
    "# 디바이스 출력\n",
    "print(f\"Device: {tensor.device}\")  # tensor가 GPU에 있는지 확인\n",
    "\n",
    "# 만약 GPU가 아닌 'cuda:0' 장치에 텐서를 명시적으로 옮기고 싶다면 아래처럼 작성\n",
    "tensor = tensor.to('cuda:0')\n",
    "\n",
    "# 다시 디바이스 출력\n",
    "print(f\"Device (명시적 이동 후): {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사용 가능한 GPU: NVIDIA GeForce RTX 2060:\n",
    "- 시스템에서 사용할 수 있는 GPU가 성공적으로 인식되었습니다.\n",
    "    Device: cuda:0:\n",
    "- 텐서가 GPU로 옮겨졌고, cuda:0 장치(첫 번째 GPU)에서 연산이 이루어지고 있음을 확인할 수 있습니다.\n",
    "    Device (명시적 이동 후): cuda:0:\n",
    "- 추가로 명시적으로 cuda:0 장치로 이동되었음을 확인하는 내용입니다. 동일하게 GPU에서 연산이 이루어지고 있음을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU 장치 지정:\n",
    "  - cuda:0은 첫 번째 GPU 장치를 의미합니다. \n",
    "  - 만약 여러 개의 GPU가 있는 시스템이라면, cuda:1, cuda:2 등으로 다른 GPU로 텐서를 이동할 수 있습니다.\n",
    "\n",
    "- 일관된 디바이스 사용:\n",
    "  - 일반적으로 모델과 데이터를 동일한 디바이스에 두어야 효율적인 연산이 가능합니다. 예를 들어, 모델이 GPU에 있다면 입력 데이터 또한 GPU로 이동시켜야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU를 사용할 수 없습니다. CPU에서 연산합니다.\n",
      "Data Device: cpu\n",
      "Model Device: cpu\n",
      "Output Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 간단한 예제 모델\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__() \n",
    "        # SimpleModel이라는 간단한 선형 모델을 정의합니다.\n",
    "        self.linear = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 데이터 생성\n",
    "sample_data = torch.rand(3, 4)\n",
    "# 샘플 데이터를 생성하고, 텐서와 모델을 device (GPU 또는 CPU)로 옮깁니다.\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"사용 가능한 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU를 사용할 수 없습니다. CPU에서 연산합니다.\")\n",
    "\n",
    "# 데이터와 모델을 디바이스로 옮기기\n",
    "sample_data = sample_data.to(device)\n",
    "model = SimpleModel().to(device)\n",
    "# 모델과 데이터의 디바이스를 출력하여 올바르게 이동되었는지 확인합니다.\n",
    "\n",
    "# 디바이스 출력\n",
    "print(f\"Data Device: {sample_data.device}\")\n",
    "print(f\"Model Device: {next(model.parameters()).device}\")\n",
    "\n",
    "# 모델 연산 실행\n",
    "output = model(sample_data)\n",
    "print(f\"Output Device: {output.device}\")\n",
    "# 모델 연산을 실행하고, 결과 텐서의 디바이스를 출력하여 연산이 올바른 디바이스에서 수행되었는지 확인합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN3ZQlftiUvA"
   },
   "source": [
    "## 2. 파이토치 설치\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이토치를 효과적으로 사용하기 위해서는 먼저 적절한 개발 환경을 설정해야 합니다. \n",
    "- 환경 설정은 크게 파이썬(Python), 파이토치 그리고 필요한 의존성 패키지를 설치하는 과정으로 나눌 수 있습니다. \n",
    "- 파이토치는 Python 3.x 버전에서 지원되므로, Python 3 이상이 설치되어 있어야 합니다. 파이썬은 공식 웹사이트(python.org)에서 다운로드할 수 있습니다. Python 배포판 Anaconda를 설치하면 실습에 필요한 패키지를 포함하고 있어서 편리합니다. 여기에서는 파이토치를 설치하기 전에 아나콘다를 설치합니다.\n",
    "- PyTorch는 공식 웹사이트(pytorch.org)에서 제공하는 명령어를 통해 설치할 수 있습니다. GPU를 사용할 경우, CUDA(Computed Unified Device Architecture) 버전에 맞는 PyTorch를 설치해야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `설치 과정 실습`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 아나콘다 설치하기 \n",
    "- https://www.anaconda.com/download\n",
    "- Skip registration 클릭 \n",
    "- 운영체제에 맞게 다운로드\n",
    "- Windows 운영체제의 경우는 Anaconda3-2024.10-1-Windows-x86_64.exe가 다운로드됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"800\" height=\"\" src=\"./image/ana_01.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 파이토치 설치하기 \n",
    "- https://pytorch.org/get-started/locally/\n",
    "- GPU를 사용하지 않을 경우 CPU 선택하고 그림 1의 Run this Command란의 설치 명령어를 Anaconda Prompt에서 실행\n",
    "- CUDA 버전을 지원하는 PyTorch를 설치하려면, 그림 2의 Run this Command란의 설치 명령어를 Anaconda Prompt에서 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/pytorch_cpu_01.PNG\" width=\"800\"/>\n",
    "<figcaption>그림 1 파이토치 설치_CPU</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/pytorch_gpu.png\" width=\"800\" height=\"\" >\n",
    "<figcaption>그림 2 파이토치 설치_GPU</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **PyTorch 버전 (`torch.__version__`)**:  \n",
    "    출력값 `2.5.1+cu118`는 현재 설치된 PyTorch의 버전 번호를 나타냅니다.\n",
    "    \n",
    "    -   `2.5.1`은 PyTorch의 주요 버전, 부 버전, 수정 버전 번호를 의미합니다. 2.5.1은 파이토치의 2.5.x 버전 시리즈 중 하나입니다.\n",
    "    -   `+cu118`은 이 버전이 CUDA 11.8을 사용하는 GPU 가속 기능이 포함되어 있음을 나타냅니다. CUDA는 NVIDIA에서 제공하는 병렬 컴퓨팅 아키텍처로, GPU의 처리 능력을 활용할 수 있게 해 주는 툴킷입니다.\n",
    "    \n",
    "-   **CUDA 사용 가능 여부 (`torch.cuda.is_available()`)**:  \n",
    "    결과값 `True`는 현재 PyTorch가 CUDA를 사용할 수 있는 환경에 구축되어 있음을 나타냅니다. 이는 다음을 의미합니다:\n",
    "    \n",
    "    -   현재 시스템에 NVIDIA GPU가 설치되어 있으며,\n",
    "    -   해당 GPU에 맞는 CUDA 드라이버가 설치되어 활성화 되어 있습니다.\n",
    "    -   PyTorch가 CUDA를 인식하고 사용할 준비가 되어 있다는 뜻입니다.\n",
    "\n",
    "-   PyTorch의 버전은 `2.5.1`으로, CUDA 11.8과 호환되며 GPU를 이용한 연산이 가능하다는 것을 확인하였습니다.\n",
    "-   CUDA가 사용 가능하므로, 딥러닝 모델을 GPU를 통해 보다 빠르게 학습하고 추론할 수 있는 환경이 마련되어 있음을 알 수 있습니다. 이는 대규모 데이터셋과 복잡한 모델을 작업할 때 성능 향상에 큰 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4146, 0.4471, 0.8197],\n",
      "        [0.2033, 0.4359, 0.6961],\n",
      "        [0.7141, 0.6594, 0.3634]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#0~1사이 실수값으로 3행 3열 텐서 생성\n",
    "x= torch.rand(3,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Colab 설치하기\n",
    "- 구글 클라우드 기반의 개발환경 Jupyter Notebook 호환\n",
    "- 데이터 과학, 머신러닝, 딥러닝 개발\n",
    "- 무료 GPU 사용(T4 GPU, TPU v2-8)\n",
    "- 구글 드라이브와 연동되어 실시간으로 데이터와 코드를 저장하고 공유할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 설치 순서\n",
    "- 구글 드라이브에 로그인 -> 더보기 -> 연결할 앱 더보기 -> Colaboratory 검색 -> 앱 설치\n",
    "- 설치 후에는 신규 -> 더보기 -> Google Colaboratory\n",
    "- 수정 -> 노트 설정 -> T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/colab_02.png\" width=\"200\" height=\"\" >\n",
    "<figcaption>그림 3 코랩 설치</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/colab_05.png\" width=\"800\" height=\"\" >\n",
    "<figcaption>그림 4 코랩 연결</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Visual Studio Code 설치\n",
    "\n",
    "- Visual Studio Code는 파이썬을 포함한 다양한 프로그래밍 언어를 지원하며, 무료입니다.\n",
    "- Visual Studio Code 다운로드 경로\n",
    "- https://code.visualstudio.com/download\n",
    "- 운영체제에 맞는 파일 다운로드하기\n",
    "- 설치 후 파이썬 관련 확장을 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/01. VSCode 설치.png\" width=\"600\" height=\"\" >\n",
    "<figcaption>그림. VS Code</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Python 환경 확인\n",
    "- 현재 사용 중인 Python 환경이 올바른지 확인해야 합니다. VSCode에서 사용하는 Python 환경이 다른 가상 환경일 수 있기 때문에, 정확한 환경을 선택해야 합니다.\n",
    "\n",
    "- VSCode에서 Python 환경 선택하기\n",
    "- VSCode 왼쪽 하단에 있는 Python 버전(혹은 환경)을 클릭합니다.\n",
    "설치된 Python 환경 목록이 나타납니다. 여기에서 원하는 환경을 선택하세요. (예: 가상 환경 또는 Anaconda 환경)\n",
    "1.2. 터미널에서 Python 환경 확인\n",
    "- VSCode에서 터미널을 열고, 아래 명령어를 실행하여 Python 환경을 확인합니다. (터미널- 새터미널)\n",
    "2. pip가 설치되어 있는지 확인\n",
    "- VSCode 터미널에서 pip이 설치되어 있는지 확인하려면 아래 명령어를 실행하세요.\n",
    "- pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 24.0 from /opt/anaconda3/lib/python3.12/site-packages/pip (python 3.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 환경변수 설정\n",
    "- Python을 설치할 때 환경 변수를 자동으로 설정하는 옵션이 있지만, 때때로 이 설정이 제대로 되지 않을 수 있습니다. Python과 pip의 실행 파일을 어디서든 사용할 수 있도록 하기 위해서는 환경 변수를 수동으로 설정해야 할 수 있습니다.\n",
    "- Python 설치 확인: \n",
    "C:\\Users\\<사용자명>\\AppData\\Local\\Programs\\Python\\Python<버전>\n",
    "- Python 3.10를 설치했다면 C:\\Users\\<사용자명>\\AppData\\Local\\Programs\\Python\\Python310일 수 있습니다.\n",
    "- 시스템 환경 변수 열기: Windows 10 이상: Windows 키를 눌러 \"환경 변수\"를 검색하고 \"시스템 환경 변수 편집\"을 클릭합니다.\n",
    "또는, 제어판 -> 시스템 -> 고급 시스템 설정 -> 환경 변수 클릭.\n",
    "- Path에 Python 경로 추가:\n",
    "\n",
    "\"시스템 변수\"에서 Path를 선택하고 편집을 클릭합니다.\n",
    "- 새로 만들기(N) 추가하고, 아래 경로들을 추가합니다:\n",
    "Python 실행 파일 경로: 예를 들어 C:\\Users\\<사용자명>\\AppData\\Local\\Programs\\Python\\Python310\n",
    "Scripts 폴더 경로: 예를 들어 C:\\Users\\<사용자명>\\AppData\\Local\\Programs\\Python\\Python310\\Scripts\n",
    "\n",
    "- 환경 변수를 설정한 후, 새로운 명령 프롬프트나 터미널을 열고 아래 명령어를 실행하여 python과 pip 명령어가 제대로 작동하는지 확인하세요.\n",
    "- python --version\n",
    "- pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2463],\n",
      "        [0.4312],\n",
      "        [0.4720],\n",
      "        [0.4312]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 간단한 신경망 모델 정의\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleNN() #.cuda()  # 모델을 GPU로 이동\n",
    "\n",
    "# 임의의 입력 데이터 (배치 크기 4)\n",
    "input_data = torch.randn(4, 2) #.cuda()  # 입력 데이터도 GPU로 이동\n",
    "\n",
    "# 출력\n",
    "output = model(input_data)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
