{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuEC8iuTv0MI"
   },
   "source": [
    "# 텐서(Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh9fMM8nv0ML"
   },
   "source": [
    "> ## 학습 목표\n",
    "\n",
    "- 파이토치 텐서의 기본 개념을 이해하고 텐서를 생성 및 활용할 수 있다.\n",
    "- 텐서의 기본 연산 및 조작 방법을 익히고, 텐서의 속성과 특성에 대해 이해할 수 있다.\n",
    "- 텐서 데이터 타입과 장치 이동 방식을 이해하고, 이를 활용하여 효율적인 프로그래밍을 할 수 있다.\n",
    "- 텐서 변환과 브로드캐스팅 개념을 이해하고, 이를 활용하여 수치 계산 및 데이터 분석을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 텐서(Tensor) 소개\n",
    "\n",
    "- 최근의 머신 러닝 시스템은 일반적으로 **텐서를 기본 데이터 구조**로 사용한다.  \n",
    "- 데이터를 위한 컨테이너라고 생각하면 쉽게 이해할 수 있다. \n",
    "- NumPy는 훌륭한 프레임워크지만, GPU를 사용하여 수치 연산을 가속화할 수는 없기 때문에 GPU연산이 가능한 PyTorch에서의 Tensor를 사용한다. (디바이스 선택이 수월함.) \n",
    " \n",
    "  심층 신경망에서 GPU는 종종 50배 또는 그 이상의 속도 향상을 제공하기 때문에, 안타깝게도 NumPy는 딥러닝 프로그래밍에는 충분치 않다.\n",
    "\n",
    "- **PyTorch 텐서(Tensor)는 개념적으로 NumPy 배열과 동일** : 텐서(Tensor)는 n-차원 배열이며, PyTorch는 이러한 텐서들의 연산을 위한 다양한 기능들을 제공한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 텐서의 특성\n",
    "\n",
    "-   축의 개수:  \n",
    "    랭크(Rank)라고도 부르며, 넘파이 배열에서는 `ndim`을 통해 확인할 수 있다.  \n",
    "    \n",
    "-   크기:  \n",
    "    텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타낸 튜플 넘파이 배열에서는 `shape`을 통해 확인할 수 있다.  \n",
    "\n",
    "-   데이터 타입:  \n",
    "    텐서에 포함된 데이터 타입이다.  \n",
    "    타입은 float32, unit8, float64 등이 될 수 있으며 문자열은 지원하지 않는다. `dtpye` 속성으로 데이터를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"800\" height=\"\" src=\"./image/tensor3.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 : https://codetorial.net/tensorflow/basics_of_tensor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# 스칼라(0D Tensor) \n",
    "# 하나의 숫자만을 담고 있는 텐서를 스칼라라고 하며 0차원텐서 라고 한다.\n",
    "# 스칼라의 축의 개수는 0개이다.\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "scalar = np.array(10)\n",
    "print(scalar)\n",
    "print(scalar.ndim)\n",
    "print(scalar.shape) # 차원이 없으므로 빈 튜플을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "1\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "# 벡터(1D Tensor) \n",
    "# 숫자들의 배열을 벡터라고 하며 1차원 텐서라고 한다.\n",
    "# 벡터의 축의 개수는 1개이다.\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "vector = np.array([1, 2, 3, 4, 5])\n",
    "print(vector)\n",
    "print(vector.ndim) # 차원 \n",
    "print(vector.shape) # 배열의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "2\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 행렬(2D Tensor)\n",
    "# 벡터들의 배열을 행렬 이라고 하며 2차원 텐서라고 한다.\n",
    "# 행렬에는 행과 열 2가지의 축이 있다.\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "print(matrix)\n",
    "print(matrix.ndim)\n",
    "print(matrix.shape) # 2행 3열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n",
      "3\n",
      "(2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "# 고차원 텐서\n",
    "# 행렬들을 하나의 새로운 배열을 합치면 숫자로 채워진 직육면체가 되는데 이는 3D Tensor이다.\n",
    "# 딥러닝에서는 보통 5차원 텐서까지 다루게 된다.\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tensor_3D = np.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "print(tensor_3D) # 3D텐서는 2개의 3*4 행렬로 구성\n",
    "print(tensor_3D.ndim) # 텐서의 차원수는 3\n",
    "print(tensor_3D.shape) # 텐서의 형태는 2개의 3*4 크기 행렬을 포함하고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Tensor의 Shape\n",
    "\n",
    "넘파이의 배열에서 `shape`를 이용하여 텐서의 크기를 알아볼 경우 return값은 다음과 같다  \n",
    "  \n",
    "**1차원 : (열,)**  \n",
    "**2차원 : (행, 열)**  \n",
    "**3차원 : (깊이, 행, 열)**  \n",
    "  \n",
    "즉, 열의 기준이 되어 차원이 늘어날수록 늘어난 차원의 크기가 열의 앞에 추가된다고 생각하면 된다.  \n",
    "머신러닝에서 사용하는 train\\_data의 shape은 다음과 같다.  \n",
    "\n",
    "**1\\. 벡터 데이터: (samples, features) 2D tensor**\n",
    "집값 예측 문제라고 생각하고 주어진 데이터가 100개의 연식, 동네, 역세권의 유무에 따른 데이터라고 하면 (100, 3)크기의 텐서에 저장될 수 있다.  \n",
    "\n",
    "**2\\. 이미지: (samples, height, width, channels) 4D tensor** \n",
    "채널 우선방식 과 채널 마지막 방식 으로 나뉘지만 보통의 경우 100 장의 28x28의 컬러 이미지라면 (100, 28, 28, 3)크기의 텐서에 저장될 수 있다.  \n",
    "\n",
    "**3\\. 동영상: (samples, frames, height, channels) 5D tenesor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.12/site-packages (3.9.0)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.12/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras) (0.14.1)\n",
      "Requirement already satisfied: ml-dtypes in /opt/anaconda3/lib/python3.12/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from keras) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mnist in /opt/anaconda3/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from mnist) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 14:12:57.299345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(train_images.shape)\n",
    "\n",
    "# 28x 28배열의 사진이 6만장이 들어있는 것을 뜻한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "use_images = train_images[:100, :, :] \n",
    "print(use_images.shape)\n",
    "\n",
    "# 일반적으로 딥러닝에서 사용하는 모든 데이터 텐서의 첫번째 축(axis0)은 샘플 축(sample axis)이다. 즉 샘플의 개수를 의미한다. 위의 use_images에서는 100을 가르키고 이는 100장의 샘플 데이터가있다는 뜻이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 샘플 축의 슬라이싱은 모델 수행 시 배치 데이터를 나눌 때 사용된다.\n",
    "\n",
    "배치 데이터를 다룰 때는 첫번째 축(axis0)를 배치축(batch axis) 또는 배치차원(batch dimension)이라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 14, 28)\n"
     ]
    }
   ],
   "source": [
    "# 6만장의 데이터 중에서 위쪽 절반만 사용하고 싶을 경우\n",
    "use_images = train_images[: , :14 , :]\n",
    "print(use_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ※ Tensor과 Ndarray\n",
    "- 텐서 (Tensor)는 NumPy 어레이와 비슷하지만, 텐서는 GPU, TPU와 같은 가속기에서 사용할 수 있고, 텐서는 값을 변경할 수 없습니다.\n",
    "\n",
    "\n",
    "먼저 Pytorch에서 주로 다루는 데이터 자료형인 Tensor과 Numpy에서 주로 다루는 데이터 자료형인 ndarray는 다차원 배열과 배열 생성 시 다양한 데이터 타입 적용이 가능하지만 주요한 차이점이 존재합니다.\n",
    "\n",
    "우선 가장 중요한 차이점은 GPU를 활용한 연산이 가능한가? 이며, 그 외로 중요한 기능들에서 차이점이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"700\" height=\"\" src=\"./image/tensor.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7FAWRY6v0MM"
   },
   "source": [
    "## 2.2 텐서 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서는 파이토치에서 스칼라, 벡터, 행렬 같은 개념으로 연산에 사용하는 기본적인 자료구조이며 다차원 배열로 표현할 수 있습니다.\n",
    "- 파이썬 numpy의 ndarray와 유사합니다.(n차원의 배열 객체)\n",
    "- 텐서는 n차원으로 구성되어 있는데 0차원은 스칼라, 1차원은 벡터, 2차원은 행렬, 3차원부터 텐서입니다. \n",
    "- 1차원 텐서, 2차원 텐서, 3차원 텐서라고도 합니다.\n",
    "- torch 패키지를 임포트하고 tensor 함수를 이용하여 텐서를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n",
      "텐서 크기: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "import torch # 파이토치 라이브러리 호출 \n",
    "# 파이토치는 머신러닝과 딥러닝을 위한 강력한 도구로, 텐서라는 특별한 데이터 구조를 사용합니다.\n",
    "\n",
    "# 1차원 텐서 생성하기\n",
    "\n",
    "a = torch.tensor([1, 2, 3, 4, 5]) #torch.tensor(대괄호 안에 원소 입력)\n",
    "# 1차원 텐서 생성, 텐서는 숫자들의 배열이라고 생각하면 됩니다. \n",
    "# 이 경우, [1, 2, 3, 4, 5]라는 다섯 개의 숫자로 이루어진 리스트를 텐서로 변환\n",
    "\n",
    "print(a)\n",
    "\n",
    "print(\"텐서 크기:\", a.size()) #텐서의 크기 확인 size()\n",
    "# 텐서 a의 크기를 출력, size() 함수는 텐서의 차원과 각 차원의 크기를 알려줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tip!` \n",
    "이 예제는 파이토치에서 가장 기본적인 데이터 구조인 텐서를 만들고 그 정보를 확인하는 방법을 보여줍니다. \n",
    "\n",
    "텐서는 숫자 데이터를 다루는 데 매우 유용하며, 머신러닝과 딥러닝에서 중요한 역할을 합니다. \n",
    "\n",
    "이러한 기초적인 개념을 이해하는 것이 더 복잡한 머신러닝 모델을 다루는 데 도움이 될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "size: torch.Size([3, 3])\n",
      "rank(차원): 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 2차원 텐서 생성하기\n",
    "# 1차원 텐서들을 바깥 대괄호로 감싸기\n",
    "\n",
    "x1 = torch.tensor([[1, 2, 3],\n",
    "                    [4, 5, 6],\n",
    "                    [7, 8, 9]])\n",
    "#  2차원 텐서는 행렬과 같은 구조를 가집니다. \n",
    "# 이 경우: 첫 번째 행: [1, 2, 3]\n",
    "#          두 번째 행: [4, 5, 6]\n",
    "#          세 번째 행: [7, 8, 9]\n",
    "# 이렇게 3개의 행과 3개의 열로 이루어진 3x3 행렬 형태의 텐서를 만듭니다.\n",
    "\n",
    "print(x1)\n",
    "\n",
    "print(\"size:\", x1.size())  # 텐서 크기 [행, 열]\n",
    "\n",
    "print(\"rank(차원):\", x1.dim())\n",
    "# 텐서의 차원 수(rank)를 출력합니다. dim() 함수는 텐서의 차원 수를 반환합니다. 이 경우 2가 출력될 것입니다. 왜냐하면 이 텐서는 2차원(행과 열)을 가지고 있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 3행 2열 리스트로 2차원 텐서 생성하기\n",
    "\n",
    "data = [[1, 2],[3, 4],[5, 6]]\n",
    "# 차원 리스트를 만듭니다. 이 리스트는 3개의 내부 리스트로 구성되어 있으며, \n",
    "# 각 내부 리스트는 2개의 숫자를 포함하고 있습니다.\n",
    "\n",
    "# 텐서로 변환\n",
    "x2 = torch.tensor(data)\n",
    "#  data 리스트를 파이토치 텐서로 변환합니다. \n",
    "# torch.tensor() 함수는 파이썬 리스트를 받아 텐서로 변환해줍니다.\n",
    "\n",
    "print(x2)\n",
    "\n",
    "print(x2.shape)\n",
    "# 텐서 x2의 형태(shape)를 출력합니다. shape 속성은 텐서의 차원과 각 차원의 크기를 알려줍니다. 결과는 torch.Size([3, 2])로 나올 것입니다. \n",
    "# 이는 이 텐서가 3행 2열의 구조를 가지고 있다는 의미입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])\n",
      "텐서 크기: torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 3차원 텐서 생성하기\n",
    "# 2차원 텐서를 대괄호로 감싸기\n",
    "\n",
    "x3 = torch.tensor([[[1, 2, 3],[4, 5, 6],[7, 8, 9]]])\n",
    "# 여기서는 3차원 텐서를 생성합니다.\n",
    "# 3차원 텐서는 '큐브' 또는 '박스'와 같은 구조를 가집니다. \n",
    "# 이 경우: 가장 바깥쪽 대괄호는 첫 번째 차원을 나타냅니다 (깊이).\n",
    "#          중간 대괄호는 두 번째 차원을 나타냅니다 (행).\n",
    "#          가장 안쪽 대괄호는 세 번째 차원을 나타냅니다 (열).\n",
    "# 이 텐서는 1개의 2차원 행렬(3x3)을 포함하고 있습니다.\n",
    "\n",
    "print(x3)\n",
    "\n",
    "print(\"텐서 크기:\", x3.size())  \n",
    "# 텐서 x3의 크기를 출력합니다. size() 함수는 텐서의 각 차원의 크기를 반환합니다. 결과는 torch.Size([1, 3, 3])로 나올 것입니다. \n",
    "# 이는 다음을 의미합니다: 첫 번째 차원(깊이)의 크기: 1\n",
    "#                         두 번째 차원(행)의 크기: 3\n",
    "#                         세 번째 차원(열)의 크기: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# NumPy 배열과 PyTorch 텐서 간의 변환을 보여주는 좋은 예시\n",
    "\n",
    "import torch\n",
    "\n",
    "# Numpy 배열로 텐서 생성하기\n",
    "import numpy as np\n",
    "# 2차원 리스트 x3_1을 NumPy 배열로 변환합니다. \n",
    "# data_np는 3x3 크기의 2차원 NumPy 배열이 됩니다.\n",
    "\n",
    "x3_1 = [[1, 2, 3],[4, 5, 6],[7, 8, 9]]\n",
    "\n",
    "data_np = np.array(x3_1)\n",
    "# torch.from_numpy() 함수를 사용하여 NumPy 배열을 PyTorch 텐서로 변환합니다. \n",
    "# 이 과정에서 데이터 타입(dtype) 정보도 함께 유지됩니다.\n",
    "\n",
    "print(data_np)\n",
    "\n",
    "x3_np = torch.from_numpy(data_np)  # dtype 정보도 포함\n",
    "\n",
    "print(x3_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 특정 값으로 텐서 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[7, 7, 7],\n",
      "        [7, 7, 7]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 0으로 채워진 텐서\n",
    "tensor_zeros = torch.zeros(2, 3)\n",
    "# 2행 3열의 2차원 텐서를 생성하고 모든 원소를 0으로 채웁니다. \n",
    "print(tensor_zeros)\n",
    "\n",
    "# 1으로 채워진 텐서\n",
    "tensor_ones = torch.ones(2, 3)\n",
    "# 2행 3열의 2차원 텐서를 생성하고 모든 원소를 1로 채웁니다.\n",
    "print(tensor_ones)\n",
    "\n",
    "# 특정값으로 채워진 텐서\n",
    "tensor_full = torch.full((2, 3), 7)\n",
    "# 2행 3열의 2차원 텐서를 생성하고 모든 원소를 7로 채웁니다. \n",
    "print(tensor_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tip!`\n",
    "- 이러한 함수들은 특정 크기와 초기값을 가진 텐서를 빠르게 생성할 때 유용합니다. \n",
    "- 예를 들어, 신경망의 가중치를 초기화하거나 마스크를 만들 때 자주 사용됩니다. 각 함수의 특징을 정리하면 다음과 같습니다:\n",
    "  - torch.zeros(): 모든 원소가 0인 텐서 생성\n",
    "  - torch.ones(): 모든 원소가 1인 텐서 생성\n",
    "  - torch.full(): 모든 원소가 지정한 값으로 채워진 텐서 생성\n",
    "- 이러한 함수들은 텐서의 크기를 인자로 받으며, torch.full()의 경우 추가로 채울 값을 인자로 받습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 random한 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "랜덤 값으로 채워진 텐서:\n",
      "tensor([[ 0.0406, -0.8622,  0.8107],\n",
      "        [ 1.7791,  0.3521, -0.2287]])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch를 사용하여 정규분포(표준 정규분포)를 따르는 랜덤 텐서를 생성하는 예제\n",
    "\n",
    "import torch\n",
    "\n",
    "# 정규분포 따른 랜덤 텐서 생성\n",
    "random_tensor = torch.randn((2, 3))\n",
    "#  torch.randn() 함수를 사용하여 2행 3열의 랜덤 텐서를 생성합니다. \n",
    "#  이 함수는 평균이 0이고 표준편차가 1인 표준 정규분포에서 무작위로 숫자를 추출합니다.\n",
    "# (2, 3)은 생성될 텐서의 형태를 지정합니다. 2행 3열의 행렬이 만들어집니다.\n",
    "\n",
    "print(\"\\n랜덤 값으로 채워진 텐서:\")\n",
    "print(random_tensor)\n",
    "# 이 결과에서 각 숫자는 표준 정규분포에서 무작위로 추출된 값입니다. 대부분의 값들은 -2와 2 사이에 분포하며, 0에 가까운 값들이 더 자주 나타납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5132, -1.0595, -0.1788],\n",
      "        [-1.6478,  0.1452,  0.2545]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 위에 생성된 random_tensor와 같은 크기인 2행 3열 랜덤 텐서 생성\n",
    "tensor_like = torch.randn_like(random_tensor)\n",
    "\n",
    "print(tensor_like)\n",
    "# torch.randn_like() 함수는 주어진 텐서와 동일한 크기와 데이터 타입을 가진 새로운 텐서를 생성합니다. 이 함수는 표준 정규 분포(평균 0, 표준편차 1)에서 무작위로 값을 추출하여 텐서를 채웁니다.\n",
    "\n",
    "# 각 원소는 -1과 1 사이의 랜덤한 값을 가집니다. 이 값들은 표준 정규 분포에서 추출되었으므로, 대부분의 값들이 -2와 2 사이에 분포하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tip!` \n",
    "\n",
    "주요 특징\n",
    "- 크기 유지: tensor_like는 random_tensor와 정확히 같은 크기(2x3)를 가집니다.\n",
    "- 독립적인 랜덤 값: 생성된 값들은 random_tensor의 값들과 완전히 독립적입니다.\n",
    "- 데이터 타입 일치: tensor_like는 random_tensor와 동일한 데이터 타입을 가집니다.\n",
    "- GPU 호환성: 만약 random_tensor가 GPU에 있다면, tensor_like도 자동으로 GPU에 생성됩니다.\n",
    "\n",
    "※ 이러한 기능은 딥러닝 모델에서 가중치 초기화나 노이즈 생성 등 다양한 상황에서 유용하게 사용될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7215, 0.7253, 0.6325],\n",
      "        [0.0186, 0.2144, 0.2634]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# rand(행, 열)은 0~1 사이의 실수값으로 채워진 텐서\n",
    "tensor_random = torch.rand(2, 3)\n",
    "# torch.rand() 함수를 사용하여 2행 3열의 텐서를 생성합니다. \n",
    "# 각 원소는 0과 1 사이의 균일 분포에서 무작위로 추출된 실수값입니다. \n",
    "# 출력 결과는 매번 다르지만 2행 3열 생성때 마다 유사한 형태일 것입니다:\n",
    "print(tensor_random)\n",
    "\n",
    "# 시작값부터 1씩증가하여 이전값까지 범위내 채워진 텐서\n",
    "tensor_arange = torch.arange(1, 7).reshape(2, 3)\n",
    "# 이 부분은 두 단계로 이루어집니다: \n",
    "# torch.arange(1, 7)은 1부터 6까지의 정수를 포함하는 1차원 텐서를 생성합니다.\n",
    "# .reshape(2, 3)은 이 1차원 텐서를 2행 3열의 2차원 텐서로 재구성합니다.\n",
    "# 출력 결과는 항상 동일합니다.\n",
    "\n",
    "print(tensor_arange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tip!`\n",
    "- rand()는 무작위 값으로 텐서를 채우는 데 유용하며, arange()와 reshape()의 조합은 특정 패턴이나 순서를 가진 텐서를 만드는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9R0MIMfv0MQ"
   },
   "source": [
    "## 2.3 텐서 속성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서의 차원을 확인하는 dim() 메서드가 있습니다. 이는 텐서의 rank라고도 불립니다.\n",
    "- 텐서의 형상(shape)을 확인하는 shape 속성이 있습니다. 이는 각 차원의 크기를 튜플 형태로 반환합니다.\n",
    "- 텐서의 크기를 확인하는 size() 메서드도 있습니다. 이는 shape와 동일한 정보를 반환합니다.\n",
    "- 데이터타입 자료형을 확인하는 dtype 속성은 datatype() 속성이며, 이를 통해 텐서의 데이터 타입(예: 정수, 실수, 불리언 등)을 알 수 있습니다.\n",
    "- 텐서가 저장된 장치(CPU 또는 GPU)를 확인하는 device 속성이 있습니다.\n",
    "- 텐서의 총 원소 개수를 반환하는 numel() 메서드도 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 3\n",
      "Shape: torch.Size([3, 4, 5])\n",
      "Size: torch.Size([3, 4, 5])\n",
      "Data type: torch.float32\n",
      "Device: cpu\n",
      "Number of elements: 60\n"
     ]
    }
   ],
   "source": [
    "# 이 코드는 3x4x5 크기의 랜덤 텐서를 생성하고 그 속성들을 출력합니다.\n",
    "# 이 코드는 텐서의 기본적인 속성들을 모두 보여주므로, PyTorch 텐서의 구조와 특성을 이해하는 데 매우 유용하게 구현되었습니다.\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.randn(3, 4, 5)\n",
    "#  3x4x5 크기의 3차원 텐서를 생성합니다. torch.randn() 함수는 표준 정규 분포(평균 0, 표준편차 1)에서 무작위로 값을 추출하여 텐서를 채웁니다.\n",
    "\n",
    "print(f\"Dimensions: {x.dim()}\")\n",
    "# dim() 메서드는 텐서의 차원 수를 반환합니다. 이 경우 3을 출력할 것입니다.\n",
    "\n",
    "print(f\"Shape: {x.shape}\")\n",
    "# shape 속성은 텐서의 각 차원의 크기를 튜플 형태로 반환합니다. 출력은 torch.Size([3, 4, 5])가 될 것입니다.\n",
    "\n",
    "print(f\"Size: {x.size()}\")\n",
    "# shape 속성은 텐서의 각 차원의 크기를 튜플 형태로 반환합니다. 출력은 torch.Size([3, 4, 5])가 될 것입니다.\n",
    "\n",
    "print(f\"Data type: {x.dtype}\")\n",
    "# dtype 속성은 텐서의 데이터 타입을 반환합니다. torch.randn()으로 생성된 텐서의 기본 데이터 타입은 torch.float32입니다.\n",
    "\n",
    "print(f\"Device: {x.device}\")\n",
    "# device 속성은 텐서가 저장된 장치(CPU 또는 GPU)를 나타냅니다. 기본적으로 'cpu'가 출력될 것입니다.\n",
    "\n",
    "print(f\"Number of elements: {x.numel()}\")\n",
    "# numel() 메서드는 텐서의 총 원소 개수를 반환합니다. 이 경우 3 * 4 * 5 = 60을 출력할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: torch.Size([5])\n",
      "shape: torch.Size([5])\n",
      "dimension: 1\n",
      "rank(차원): 1\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# PyTorch를 사용하여 1차원 텐서를 생성하고 그 속성들을 출력하는 예제\n",
    "import torch\n",
    "\n",
    "# 1차원 텐서\n",
    "# 텐서의 크기 확인 size(), shape\n",
    "# 차원 확인 dim(), ndimension()\n",
    "# 데이터타입 dtype()\n",
    "a = torch.tensor([1, 2, 3, 4, 5])\n",
    "#  1차원 텐서 a를 생성합니다. 이 텐서는 5개의 정수 원소를 포함합니다.\n",
    "\n",
    "print(\"size:\", a.size())\n",
    "#  1차원 텐서 a를 생성합니다. 이 텐서는 5개의 정수 원소를 포함합니다.\n",
    "\n",
    "print(\"shape:\", a.shape) # 5개 원소\n",
    "# shape 속성도 텐서의 크기를 반환합니다. size()와 동일한 정보를 제공합니다.\n",
    "\n",
    "print(\"dimension:\", a.dim())\n",
    "# shape 속성도 텐서의 크기를 반환합니다. size()와 동일한 정보를 제공합니다.\n",
    "\n",
    "print(\"rank(차원):\", a.ndimension())\n",
    "# shape 속성도 텐서의 크기를 반환합니다. size()와 동일한 정보를 제공합니다.\n",
    "\n",
    "print(a.dtype)\n",
    "# dtype 속성은 텐서의 데이터 타입을 반환합니다. 이 경우 torch.int64를 출력할 것입니다. 이는 64비트 정수형을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: torch.Size([2, 2])\n",
      "shape: torch.Size([2, 2])\n",
      "dimension: 2\n",
      "rank(차원): 2\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "#  PyTorch를 사용하여 2차원 실수형 텐서를 생성하고 그 속성들을 출력하는 예제\n",
    "import torch\n",
    "\n",
    "# 실수형 FloatTensor\n",
    "x4 = torch.FloatTensor([[1, 2],[3, 4]])\n",
    "# 2x2 크기의 실수형 텐서 x4를 생성\n",
    "# FloatTensor는 32비트 부동소수점 데이터 타입을 사용\n",
    "\n",
    "print(\"size:\", x4.size())\n",
    "# size() 메서드는 텐서의 크기를 반환\n",
    "# 출력은 torch.Size([2, 2])\n",
    "\n",
    "print(\"shape:\", x4.shape)\n",
    "# size() 메서드는 텐서의 크기를 반환\n",
    "# 출력은 torch.Size([2, 2])\n",
    "\n",
    "print(\"dimension:\", x4.dim())\n",
    "# size() 메서드는 텐서의 크기를 반환\n",
    "\n",
    "print(\"rank(차원):\", x4.ndimension())\n",
    "# size() 메서드는 텐서의 크기를 반환\n",
    "\n",
    "print(x4.dtype)\n",
    "# dtype 속성은 텐서의 데이터 타입을 반환\n",
    "# torch.float32를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 3, 3])\n",
      "dimension: 3\n",
      "rank(차원): 3\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# PyTorch를 사용하여 3차원 텐서를 생성하고 그 속성들을 출력하는 예제\n",
    "import torch\n",
    "\n",
    "# 3차원 텐서\n",
    "x5 = torch.tensor([[[1, 2, 3],[4, 5, 6],[7, 8, 9]]])\n",
    "# 3차원 텐서 x5를 생성합니다. 이 텐서는 1x3x3 크기를 가집니다.\n",
    "\n",
    "print(\"shape:\", x5.shape)\n",
    "# shape 속성은 텐서의 각 차원의 크기를 튜플 형태로 반환\n",
    "# 출력은 torch.Size([1, 3, 3]), 이는 1개의 3x3 행렬을 포함하는 3차원 텐서를 의미\n",
    "\n",
    "print(\"dimension:\", x5.dim())\n",
    "# dim() 메서드는 텐서의 차원 수를 반환\n",
    "# 이 경우 3을 출력\n",
    "\n",
    "print(f\"rank(차원): {x5.ndimension()}\")\n",
    "# ndimension() 메서드는 dim()과 동일한 기능을 합니다. \n",
    "# 역시 3을 출력\n",
    "\n",
    "print(x5.dtype)\n",
    "# dtype 속성은 텐서의 데이터 타입을 반환\n",
    "# torch.int64를 출력, 이는 64비트 정수형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `텐서생성 실습`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 예제는 행렬 곱셈의 기본 원리를 보여줍니다. \n",
    "- 첫 번째 행렬의 열 수(3)와 두 번째 행렬의 행 수(3)가 일치해야 행렬 곱셈이 가능합니다. \n",
    "- 결과 텐서의 크기는 첫 번째 행렬의 행 수(2)와 두 번째 행렬의 열 수(2)로 결정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 58,  64],\n",
      "        [139, 154]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# [2, 3]과 [3, 2] 텐서 생성하기\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
    "\n",
    "c = torch.matmul(a, b)\n",
    "# c = torch.matmul(a, b)는 a와 b의 행렬 곱셈을 수행합니다.\n",
    "# 이 함수는 내부적으로 최적화된 알고리즘을 사용하여 빠른 계산을 수행합니다. \n",
    "\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드는 두 텐서 a와 b의 행렬 곱셈을 수행합니다. 행렬 곱셈 과정을 자세히 설명하겠습니다:\n",
    "- 텐서 a는 2x3 크기이고, 텐서 b는 3x2 크기입니다.\n",
    "- 행렬 곱셈 규칙에 따라, 첫 번째 행렬의 열 수(3)와 두 번째 행렬의 행 수(3)가 일치해야 합니다.\n",
    "- 결과 텐서 c의 크기는 첫 번째 행렬의 행 수(2)와 두 번째 행렬의 열 수(2)가 됩니다. 따라서 c는 2x2 크기가 됩니다.\n",
    "- 행렬 곱셈 과정:\n",
    "  - c[0,0] = a[0,0]b[0,0] + a[0,1]b[1,0] + a[0,2]b[2,0]\n",
    "    = 17 + 29 + 311 = 7 + 18 + 33 = 58\n",
    "  - c[0,1] = a[0,0]b[0,1] + a[0,1]b[1,1] + a[0,2]b[2,1]\n",
    "    = 18 + 210 + 312 = 8 + 20 + 36 = 64\n",
    "  - c[1,0] = a[1,0]b[0,0] + a[1,1]b[1,0] + a[1,2]b[2,0]\n",
    "    = 47 + 59 + 611 = 28 + 45 + 66 = 139\n",
    "  - c[1,1] = a[1,0]b[0,1] + a[1,1]b[1,1] + a[1,2]b[2,1]\n",
    "    = 48 + 510 + 612 = 32 + 50 + 72 = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: tensor([[0.1504, 0.5052, 0.5347],\n",
      "        [0.3169, 0.7463, 0.5682],\n",
      "        [0.3060, 0.1419, 0.6532]])\n",
      "Reshaped: tensor([[0.1504, 0.5052, 0.5347, 0.3169, 0.7463, 0.5682, 0.3060, 0.1419, 0.6532]])\n",
      "Transposed: tensor([[0.1504, 0.3169, 0.3060],\n",
      "        [0.5052, 0.7463, 0.1419],\n",
      "        [0.5347, 0.5682, 0.6532]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 다양한 방법으로 텐서 생성\n",
    "tensor1 = torch.tensor([1, 2, 3]) \n",
    "# 주어진 데이터로 1차원 텐서를 생성,결과는 [1, 2, 3] 형태의 1차원 텐서\n",
    "tensor2 = torch.zeros(3, 3)\n",
    "# 3x3 크기의 모든 원소가 0인 2차원 텐서 생성\n",
    "tensor3 = torch.ones(2, 2)\n",
    "# 2x2 크기의 모든 원소가 1인 2차원 텐서 생성\n",
    "tensor4 = torch.rand(3, 3)\n",
    "# 3x3 크기의 0과 1 사이의 균일 분포에서 무작위로 값을 추출하여 2차원 텐서를 생성\n",
    "\n",
    "# 텐서 조작\n",
    "reshaped = tensor4.reshape(1, 9)\n",
    "# 3x3 크기의 0과 1 사이의 균일 분포에서 무작위로 값을 추출하여 2차원 텐서를 생성\n",
    "transposed = tensor4.t()\n",
    "# tensor4의 전치(transpose)를 수행합니다. 행과 열을 바꾸어 새로운 텐서를 생성\n",
    "\n",
    "print(\"Original:\", tensor4)\n",
    "print(\"Reshaped:\", reshaped)\n",
    "print(\"Transposed:\", transposed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력 결과\n",
    "  - 출력 결과는 다음과 같은 형태일 것입니다:\n",
    "  - \"Original:\" - 원본 tensor4를 출력합니다. 3x3 크기의 랜덤 값을 가진 텐서입니다.\n",
    "  - \"Reshaped:\" - tensor4를 1x9 형태로 재구성한 텐서를 출력합니다.\n",
    "  - \"Transposed:\" - tensor4의 전치 결과를 출력합니다. 원본 텐서의 행과 열이 바뀐 형태입니다.\n",
    "- 이 예제는 PyTorch에서 텐서를 생성하고 조작하는 기본적인 방법들을 보여줍니다. \n",
    "- 이러한 연산들은 딥러닝 모델을 구축하고 데이터를 처리할 때 자주 사용됩니다1\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 타입을 지정 텐서 생성\n",
    "- 정수형 텐서나 실수형 텐서를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "실수형 텐서:\n",
      "tensor([1.5000, 2.5000, 3.5000])\n",
      "\n",
      "정수형 텐서:\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch를 사용하여 실수형과 정수형 텐서를 생성하는 예제\n",
    "# 이 코드는 PyTorch에서 다양한 데이터 타입의 텐서를 생성하는 방법을 보여줍니다. 데이터 타입을 명시적으로 지정함으로써, 메모리 사용과 연산 정확도를 조절할 수 있습니다. 실수형 텐서는 주로 연속적인 값이나 가중치를 표현할 때 사용되며, 정수형 텐서는 카운트나 인덱스와 같은 이산적인 값을 표현할 때 사용됩니다.\n",
    "\n",
    "import torch\n",
    "\n",
    "# 실수형 텐서 생성 (float32)\n",
    "float_tensor = torch.tensor([1.5, 2.5, 3.5], dtype=torch.float32)\n",
    "print(\"\\n실수형 텐서:\")\n",
    "print(float_tensor)\n",
    "# 이 부분은 32비트 부동소수점(float32) 데이터 타입의 1차원 텐서를 생성합니다.\n",
    "# [1.5, 2.5, 3.5]: 텐서의 초기 값들입니다. \n",
    "# dtype=torch.float32: 텐서의 데이터 타입을 32비트 부동소수점으로 지정합니다.\n",
    "\n",
    "\n",
    "# 정수형 텐서 생성 (int64)\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
    "print(\"\\n정수형 텐서:\")\n",
    "print(int_tensor)\n",
    "# 이 부분은 32비트 부동소수점(float32) 데이터 타입의 1차원 텐서를 생성합니다.\n",
    "# [1.5, 2.5, 3.5]: 텐서의 초기 값들입니다.\n",
    "# dtype=torch.float32: 텐서의 데이터 타입을 32비트 부동소수점으로 지정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 텐서 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이토치는 앞에서 언급했듯이 머신러닝, 딥러닝 오픈소스 라이브러리입니다.\n",
    "- 딥러닝을 이해하기 위해서 텐서의 기본적인 연산인 텐서 덧셈, 곱셈, 행렬곱셈 방법을 알아보겠습니다.\n",
    "- 행렬곱은 왼쪽 행렬의 열의 수와 오른쪽 행렬의 행의 수가 일치해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기본 연산\n",
    "\n",
    "-   **덧셈과 뺄셈**: 같은 크기의 텐서 간에 요소별로 수행됩니다.\n",
    "-   **요소별 곱셈**: 두 텐서의 대응하는 요소끼리 곱합니다.\n",
    "-   **스칼라 곱**: 텐서의 모든 요소에 스칼라 값을 곱합니다.\n",
    "\n",
    "#### 행렬 곱셈\n",
    "\n",
    "-   행렬 곱셈은 `torch.matmul()` 또는 `@` 연산자를 사용합니다.\n",
    "-   왼쪽 행렬의 열 수와 오른쪽 행렬의 행 수가 일치해야 합니다.\n",
    "-   결과 행렬의 크기는 (왼쪽 행렬의 행 수) x (오른쪽 행렬의 열 수)가 됩니다.\n",
    "\n",
    "#### 고급 연산\n",
    "\n",
    "-   **전치(Transpose)**: 행과 열을 바꾸는 연산으로, `t()` 메서드를 사용합니다.\n",
    "-   **차원 축소**: `sum()`, `mean()` 등의 함수로 특정 차원을 따라 값을 집계합니다.\n",
    "-   **브로드캐스팅**: 크기가 다른 텐서 간의 연산을 자동으로 조정합니다.\n",
    "\n",
    "#### 비선형 활성화 함수\n",
    "\n",
    "-   ReLU, Sigmoid, Tanh 등의 함수를 텐서에 적용하여 비선형성을 도입합니다.\n",
    "\n",
    "이러한 연산들은 신경망의 순전파와 역전파 과정에서 핵심적인 역할을 합니다. \n",
    "\n",
    "효율적인 텐서 연산은 딥러닝 모델의 성능과 학습 속도에 직접적인 영향을 미칩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1차 텐서 덧셈\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "c = a + b \n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 텐서 a와 b의 덧셈 풀이는 다음과 같습니다. \n",
    "- [1+4, 2+5, 3+6] = [5, 7, 9] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3, -3, -3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1차 텐서 뺄셈\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "c = a - b \n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 텐서 a와 b의 뺄셈 풀이는 다음과 같습니다. \n",
    "- [1-4, 2-5, 3-6] = [-3, -3, -3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: tensor([5, 7, 9])\n",
      "Multiplication: tensor([ 4, 10, 18])\n",
      "Dot Product: tensor(32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# 기본 연산\n",
    "addition = a + b\n",
    "multiplication = a * b\n",
    "dot_product = torch.dot(a, b)\n",
    "\n",
    "print(\"Addition:\", addition)\n",
    "print(\"Multiplication:\", multiplication)\n",
    "print(\"Dot Product:\", dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor = torch.rand(3, 3).to(device)\n",
    "print(f\"Tensor is on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 PyTorch를 사용하여 텐서를 생성하고 GPU가 사용 가능한 경우 GPU로 텐서를 이동시키는 예제입니다. 각 줄을 자세히 설명하겠습니다:\n",
    "\n",
    "1.  `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`  \n",
    "    이 줄은 GPU 사용 가능 여부를 확인하고 적절한 장치를 선택합니다:\n",
    "    \n",
    "    -   `torch.cuda.is_available()`은 CUDA(GPU 연산을 위한 NVIDIA의 병렬 컴퓨팅 플랫폼)가 사용 가능한지 확인합니다.\n",
    "    -   GPU가 사용 가능하면 `device`는 \"cuda\"로 설정되고, 그렇지 않으면 \"cpu\"로 설정됩니다.\n",
    "    \n",
    "2.  `tensor = torch.rand(3, 3).to(device)`  \n",
    "    이 줄은 3x3 크기의 랜덤 텐서를 생성하고 선택된 장치로 이동시킵니다:\n",
    "    \n",
    "    -   `torch.rand(3, 3)`는 0과 1 사이의 균일 분포에서 무작위로 값을 추출하여 3x3 텐서를 생성합니다.\n",
    "    -   `.to(device)`는 생성된 텐서를 선택된 장치(GPU 또는 CPU)로 이동시킵니다.\n",
    "    \n",
    "3.  `print(f\"Tensor is on {device}\")`  \n",
    "    이 줄은 텐서가 어떤 장치에 있는지 출력합니다.\n",
    "\n",
    "이 코드는 GPU가 사용 가능한 경우 자동으로 GPU를 활용하여 연산을 수행할 수 있게 해줍니다. GPU를 사용하면 대규모 텐서 연산과 딥러닝 모델 학습 속도를 크게 향상시킬 수 있습니다. GPU가 없는 시스템에서는 자동으로 CPU를 사용하므로, 코드의 호환성도 유지됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 곱셈\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "c = torch.mul(a, b)  # 요소별 곱셈\n",
    "c1 = torch.matmul(a, b) # 행렬곱\n",
    "print(c)\n",
    "print(c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행렬곱 matmul 주의할 점은 행렬 a의 열의 수와 b의 행의 수가 같아야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/2.1_matmul.png\" width=\"800\"/>\n",
    "<figcaption>그림 2.1 텐서 행렬곱</figcaption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a = torch.tensor([10, 20, 30])\n",
    "b = torch.tensor([2, 4, 5])\n",
    "\n",
    "result = a / b\n",
    "# 또는 torch.div(a, b) 사용 가능\n",
    "\n",
    "# a를 b로 요소별로 나눕니다. 이는 다음과 같이 계산됩니다 \n",
    "# 10 / 2 = 5 \n",
    "# 20 / 4 = 5 \n",
    "# 30 / 5 = 6\n",
    "# 참고: torch.div(a, b)를 사용해도 동일한 결과를 얻을 수 있습니다.\n",
    "\n",
    "print(result)  # tensor([5., 5., 6.]) 결과가 부동소수점 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  9, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a = torch.tensor([2, 3, 4])\n",
    "\n",
    "# 텐서 요소 각각의 거듭제곱\n",
    "result = a ** 2\n",
    "# 또는 torch.pow(a, 2)\n",
    "# 2^2 = 4, 3^2 = 9, 4^2 = 16\n",
    "\n",
    "print(result)  # tensor([ 4,  9, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21)\n",
      "tensor([5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 전체 합계\n",
    "sum_value = torch.sum(tensor)\n",
    "print(sum_value)  # tensor(21) 1 + 2 + 3 + 4 + 5 + 6 = 21\n",
    "\n",
    "# 특정 차원의 합계\n",
    "sum_value_dim0 = torch.sum(tensor, dim=0)  # 행을 기준으로 열의 합\n",
    "#  dim=0 매개변수를 사용하여 행을 기준으로 열의 합을 계산합니다. \n",
    "# 결과는 [5, 7, 9]가 됩니다. 계산 과정은 다음과 같습니다 \n",
    "# 첫 번째 열: 1 + 4 = 5\n",
    "# 두 번째 열: 2 + 5 = 7 \n",
    "# 세 번째 열: 3 + 6 = 9 \n",
    "# 이러한 합계 연산은 딥러닝에서 자주 사용됩니다. \n",
    "# 예를 들어, 손실 함수의 계산이나 특성 집계 등에 활용될 수 있습니다.\n",
    "\n",
    "print(sum_value_dim0)  # tensor([5, 7, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# 최댓값\n",
    "max_value = torch.max(tensor)\n",
    "print(max_value)  # tensor(5)\n",
    "\n",
    "# 최솟값\n",
    "min_value = torch.min(tensor)\n",
    "print(min_value)  # tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "# 비교 연산은 데이터 필터링, 조건부 연산, 마스킹 등 다양한 상황에서 유용하게 사용됩니다. \n",
    "# 예를 들어, 이 결과를 사용하여 원본 텐서에서 3보다 큰 값만 선택하거나, 특정 조건을 만족하는 데이터에 대해서만 연산을 수행할 수 있습니다.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# 3보다 큰 값들만 True\n",
    "result = tensor > 3\n",
    "# 첫 세 요소(1, 2, 3)는 3보다 크지 않으므로 False입니다.\n",
    "# 마지막 두 요소(4, 5)는 3보다 크므로 True입니다.\n",
    "\n",
    "print(result)  # tensor([False, False, False,  True,  True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 텐서 연산을 위한 텐서 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서 연산은 필요에 따라 차원(dimensions)을 변경할 수 있습니다.\n",
    "- view(), unsqueeze(), squeeze() 함수를 이용하여 텐서 변환을 할 수 있습니다.\n",
    "- 중요한건 차원크기가 무조건 1로 늘어나거나 1인 것만 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exam텐서의 차원 : torch.Size([3, 1, 4, 1])\n",
      "exam텐서의 차원 : torch.Size([3, 4, 1])\n",
      "exam텐서의 차원 : torch.Size([3, 4])\n",
      "exam텐서의 차원 : torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "exam = torch.rand(3,1,4,1)\n",
    "print(f\"exam텐서의 차원 : {exam.shape}\")\n",
    "\n",
    "#3, 1, 4, 1차원의 idx는 0, 1, 2, 3\n",
    "exam = exam.squeeze(dim=1) \n",
    "print(f\"exam텐서의 차원 : {exam.shape}\")\n",
    "\n",
    "# idx는 리스트 인덱싱처럼 뒤에서도 순번을 지정\n",
    "exam = exam.squeeze(dim=-1) \n",
    "print(f\"exam텐서의 차원 : {exam.shape}\")\n",
    "\n",
    "# 차원을 늘리는것은 인자값에 해당하는 순번에 `1`로 빈 차원이 삽입\n",
    "exam = exam.unsqueeze(dim=0)\n",
    "print(f\"exam텐서의 차원 : {exam.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"700\" height=\"\" src=\"./image/tensor2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Tensor`자료형의 '확장'은 무조건 차원 확장 후 → 차원값 확장 으로 진행되기 때문이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `torch.repeat()`, `torch.expand()`\n",
    "\n",
    "위에서 차원 확장&축소와 관련된 메서드 `squeeze()` & `unsqueeze()`를 설명했으니 이제 확장된 차원크기를 조정하는 메서드를 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "tensor([[[0, 1],\n",
      "         [2, 1],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [2, 1],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [2, 1],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [2, 1],\n",
      "         [2, 0]]])\n"
     ]
    }
   ],
   "source": [
    "# 알아보기 슆게 임의 데이터 하나를 난수(정수 데이터타입)\n",
    "origin_data = torch.randint(0, 4, (3,2))\n",
    "\n",
    "# 차원 확장 [3, 2] -> [1, 3, 2]\n",
    "temp_data = origin_data.unsqueeze(0)\n",
    "\n",
    "# 확장된 차원을 반복하여 늘림 [1, 3, 2] -> [4, 3, 2]\n",
    "exp_res_data = temp_data.expand(4, 3, 2)\n",
    "\n",
    "# 차원 사이즈가 어떻게 늘어났는지 확인\n",
    "print(exp_res_data.shape)\n",
    "\n",
    "# 원소가 어떻게 늘어났는지 확인하기\n",
    "print(exp_res_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "tensor([[[0, 1],\n",
      "         [2, 1],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [2, 1],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [2, 1],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [2, 1],\n",
      "         [2, 0]]])\n"
     ]
    }
   ],
   "source": [
    "# 확장된 차원을 repeat메서드로 늘림\n",
    "rep_res_data = temp_data.repeat(4, 1, 1)\n",
    "\n",
    "# 차원 사이즈가 어떻게 늘어났는지 확인\n",
    "print(rep_res_data.shape)\n",
    "\n",
    "# 원소가 어떻게 늘어났는지 확인하기\n",
    "print(rep_res_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`expand`는 차원 사이즈가 늘어나도 메모리는 공유되고 있기에  \n",
    "하나의 원소값을 변조하면 공유 메모리로 묶여있는 다른 원소도 같이 영향받는다.\n",
    "\n",
    "따라서 `expand`로 차원을 늘리는 경우는 브로드 캐스팅 방식으로 읽기전용 데이터를 만들 때 최적화 측면에서 사용하는 메서드라 보면 된다.\n",
    "\n",
    "반대로 `repeat`메서드는 차원사이즈가 늘어나면 복제된 데이터는 완전히 독립적으로 운용되어 원소 변조가 발생하는 쓰기데이터일 때 위 메서드를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 주요 텐서 변환 함수\n",
    "\n",
    "#### view() 함수\n",
    "\n",
    "-   텐서의 크기를 변경하는 데 사용됩니다.\n",
    "-   변경 후에도 텐서 원소의 총 개수는 동일해야 합니다.\n",
    "-   예: `x1_tensor = x_tensor.view(2, 3)`는 3x2 텐서를 2x3 텐서로 변환합니다.\n",
    "\n",
    "#### unsqueeze() 함수\n",
    "\n",
    "-   지정된 위치에 새로운 차원을 추가합니다.\n",
    "-   `dim` 매개변수로 차원을 추가할 위치를 지정합니다.\n",
    "-   예: `uns_tensor = x_tensor.unsqueeze(0)`는 첫 번째 위치에 새 차원을 추가합니다.\n",
    "\n",
    "#### squeeze() 함수\n",
    "\n",
    "-   크기가 1인 차원을 제거합니다.\n",
    "-   특정 연산 후 불필요한 차원을 제거할 때 유용합니다.\n",
    "-   예: `s_tensor = uns_tensor.squeeze()`는 크기가 1인 모든 차원을 제거합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 추가적인 텐서 변환 기능\n",
    "\n",
    "#### chunk() 함수\n",
    "\n",
    "-   텐서를 지정된 수의 덩어리로 나눕니다.\n",
    "-   예: `chunks = torch.chunk(tensor, 3)`는 텐서를 3개의 덩어리로 나눕니다.\n",
    "\n",
    "#### split() 함수\n",
    "\n",
    "-   텐서를 지정된 크기의 덩어리로 나눕니다.\n",
    "-   예: `splits = torch.split(tensor, 2)`는 텐서를 2개의 원소를 가진 덩어리들로 나눕니다.\n",
    "\n",
    "이러한 텐서 변환 함수들은 딥러닝 모델 구축 및 데이터 전처리 과정에서 매우 유용하게 사용됩니다. \n",
    "\n",
    "특히 복잡한 신경망 구조에서 텐서의 형태를 조정하거나, 배치 처리를 위해 데이터의 차원을 조정할 때 자주 활용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0889,  0.7156],\n",
      "        [ 0.7348,  0.7041],\n",
      "        [ 0.4144, -0.0938]])\n",
      "tensor([[-1.0889,  0.7156,  0.7348],\n",
      "        [ 0.7041,  0.4144, -0.0938]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# view() 함수는 텐서의 크기를 변경하는 데 사용됩니다.\n",
    "# 변경 후에도 텐서 원소의 개수는 같아야 합니다.\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# randn() 함수를 사용해 정규분포 텐서 생성하기\n",
    "# 3x2 텐서를 2x3 텐서로 변환하기\n",
    "x_tensor = torch.randn(3, 2)\n",
    "# 이 함수는 평균이 0이고 표준편차가 1인 정규분포에서 무작위로 값을 추출하여 텐서를 채웁니다.\n",
    "\n",
    "# 텐서 크기 변경\n",
    "x1_tensor = x_tensor.view(2,3)\n",
    "# view() 함수는 텐서의 형태를 변경합니다. 여기서는 3x2 텐서를 2x3 텐서로 변환합니다. 이 변환은 텐서의 총 원소 수(6개)를 유지하면서 수행됩니다.\n",
    "\n",
    "# view() 함수는 텐서의 데이터를 실제로 복사하지 않고 같은 데이터를 다른 형태로 보는 방식으로 작동합니다. 이는 메모리 효율적이며, 원본 텐서와 변형된 텐서가 같은 메모리를 공유함을 의미합니다.\n",
    "\n",
    "print(x_tensor)\n",
    "print(x1_tensor)\n",
    "print(x1_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8108, -1.8906],\n",
      "        [ 1.2298,  0.1716],\n",
      "        [-0.0889,  1.4484]])\n",
      "tensor([ 0.8108, -1.8906,  1.2298,  0.1716, -0.0889,  1.4484])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# 이 예제는 텐서의 형태를 변경하는 방법을 보여주며, 데이터 처리나 신경망 구조 설계 시 자주 사용되는 기법입니다.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 2차원 텐서\n",
    "x_tensor = torch.randn(3, 2)\n",
    "# torch.randn(3, 2)는 3행 2열의 2차원 텐서를 생성합니다. 이 함수는 표준 정규 분포(평균 0, 표준편차 1)에서 무작위로 값을 추출하여 텐서를 채웁니다.\n",
    "\n",
    "# 1차원으로 텐서 차원 변경\n",
    "x1_tensor = x_tensor.view(6)\n",
    "# torch.randn(3, 2)는 3행 2열의 2차원 텐서를 생성합니다. 이 함수는 표준 정규 분포(평균 0, 표준편차 1)에서 무작위로 값을 추출하여 텐서를 채웁니다.\n",
    "# x_tensor는 원본 2차원 텐서를 보여주고, x1_tensor는 같은 데이터를 1차원으로 펼친 형태를 보여줍니다. x1_tensor.shape는 변환된 텐서의 크기가 6인 1차원 텐서임을 나타냅니다.\n",
    "\n",
    "print(x_tensor)\n",
    "print(x1_tensor)\n",
    "print(x1_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "tensor([3, 4])\n",
      "tensor([5, 6])\n"
     ]
    }
   ],
   "source": [
    "#  PyTorch를 사용하여 1차원 텐서를 여러 개의 작은 텐서로 나누는 예제\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1x6 텐서를 3개의 텐서로 나누기\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "# 1부터 6까지의 정수를 포함하는 1차원 텐서를 생성\n",
    "    \n",
    "# 텐서 분할\n",
    "chunks = torch.chunk(tensor, 3)  # 3개의 텐서로 나눔\n",
    "# torch.chunk() 함수는 텐서를 지정된 수의 덩어리로 나눕니다. \n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "# tensor([1, 2])\n",
    "# tensor([3, 4])\n",
    "# tensor([5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.chunk()` 함수의 특징:\n",
    "\n",
    "-   가능한 한 균등하게 텐서를 나누려고 시도합니다.\n",
    "-   텐서의 크기가 chunk 수로 나누어 떨어지지 않을 경우, 마지막 chunk가 더 작을 수 있습니다.\n",
    "-   각 chunk는 원본 텐서의 뷰(view)이므로, 메모리를 추가로 사용하지 않습니다.\n",
    "\n",
    "이 기능은 대규모 텐서를 처리할 때나 병렬 처리를 위해 데이터를 나눌 때 유용하게 사용될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "tensor([3, 4])\n",
      "tensor([5, 6])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch를 사용하여 1차원 텐서를 여러 개의 작은 텐서로 나누는 예제\n",
    "# torch.split() 함수를 사용하여 텐서를 분할\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1x6 텐서를 2개의 원소를 가지는 텐서로 나누기\n",
    "splits = torch.split(tensor, 2)  # 각 텐서가 2개의 원소를 가짐\n",
    "# torch.split() 함수는 텐서를 지정된 크기의 덩어리로 나눕니다. \n",
    "\n",
    "for split in splits:\n",
    "    print(split)\n",
    "# tensor([1, 2])\n",
    "# tensor([3, 4])\n",
    "# tensor([5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.split()` 함수의 특징:\n",
    "\n",
    "-   지정된 크기(여기서는 2)만큼 텐서를 균등하게 나눕니다.\n",
    "-   마지막 덩어리가 지정된 크기보다 작을 수 있습니다(이 예제에서는 모든 덩어리가 동일한 크기입니다).\n",
    "-   각 분할된 텐서는 원본 텐서의 뷰(view)이므로, 메모리를 추가로 사용하지 않습니다.\n",
    "\n",
    "이 기능은 대규모 텐서를 처리할 때나 배치 처리를 위해 데이터를 나눌 때 유용하게 사용될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 차원 추가\n",
    "\n",
    "- unsqueeze() 함수는 차원을 추가하는 기능을 합니다. unsqueeze(x_tensor, dim)\n",
    "- dim을 0이라고 지정하면 텐서 크기에서 인덱스 0 즉, 첫 번째 자리에 차원을 추가하라는 뜻입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차원추가: tensor([[[-0.1960, -0.4731],\n",
      "         [-0.5035, -1.4313],\n",
      "         [ 0.8567, -1.2912]]])\n",
      "torch.Size([1, 3, 2])\n",
      "차원추가: tensor([[[-0.1960, -0.4731]],\n",
      "\n",
      "        [[-0.5035, -1.4313]],\n",
      "\n",
      "        [[ 0.8567, -1.2912]]])\n",
      "torch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 예시 텐서 생성\n",
    "x_tensor = torch.randn(3, 2)  # 3x2 크기의 랜덤 텐서 생성\n",
    "\n",
    "# 차원 추가 unsqueeze(x_tensor, dim)\n",
    "\n",
    "# dim = 0은 차원을 추가할 첫 번째 자리 (가장 바깥쪽)\n",
    "uns_tensor = x_tensor.unsqueeze(0)\n",
    "# x_tensor의 첫 번째 위치(가장 바깥쪽)에 새로운 차원을 추가\n",
    "\n",
    "print(f'차원추가: {uns_tensor}')\n",
    "print(uns_tensor.shape)\n",
    "\n",
    "# dim = 1은 차원을 추가할 두 번째 자리\n",
    "uns1_tensor = x_tensor.unsqueeze(1)\n",
    "# x_tensor의 두 번째 위치에 새로운 차원을 추가\n",
    "\n",
    "print(f'차원추가: {uns1_tensor}')\n",
    "print(uns1_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- squeeze() 함수는 텐서에서 크기가 1인 차원을 제거하는 기능을 합니다. \n",
    "- 특정 연산 후 불필요한 차원을 제거할 때 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차원축소: tensor([[-0.1960, -0.4731],\n",
      "        [-0.5035, -1.4313],\n",
      "        [ 0.8567, -1.2912]])\n",
      "torch.Size([3, 2])\n",
      "차원축소: tensor([[-0.1960, -0.4731],\n",
      "        [-0.5035, -1.4313],\n",
      "        [ 0.8567, -1.2912]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 차원 축소\n",
    "s_tensor = uns_tensor.squeeze()\n",
    "print(f'차원축소: {s_tensor}')\n",
    "print(s_tensor.shape)\n",
    "\n",
    "s1_tensor = uns1_tensor.squeeze()\n",
    "print(f'차원축소: {s1_tensor}')\n",
    "print(s1_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 브로드캐스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 브로드캐스팅(broadcasting)은 두 텐서 간의 연산이 발생할 때, 텐서의 크기가 일치하지 않으면 연산이 가능하도록 자동으로 크기를 맞춰줍니다.\n",
    "- 차원 크기가 1인 텐서는 그 차원의 크기를 다른 텐서의 크기에 맞춰 확장됩니다.\n",
    "- 브로드캐스팅은 실제로 텐서를 확장하지 않고 연산만을 처리하기 때문에 메모리 낭비를 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 두 텐서의 차원 크기가 같거나 1차원 \n",
    "### b의 텐서 크기가 a의 텐서 크기에 자동으로 맞춰진다.\n",
    "a = torch.tensor([[1, 2],[3, 5]])\n",
    "b = torch.tensor([1, 2])\n",
    "\n",
    "c = a + b # (2, 2) + (2, ) -> (2, 2)로 브로드캐스팅\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  4],\n",
      "        [ 3, 10]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np  \n",
    "\n",
    "a = torch.tensor([[1, 2],[3, 5]]) # 2x2 크기의 2차원 텐서 생성\n",
    "b = torch.tensor([1, 2]) # 1차원 텐서 생성\n",
    "c = a * b \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 PyTorch를 사용하여 텐서 간의 브로드캐스팅 곱셈 연산을 수행합니다. 각 부분을 자세히 설명하겠습니다:\n",
    "\n",
    "**1.  텐서 생성 :**\n",
    "\n",
    "- a는 2x2 크기의 2차원 텐서입니다: \\[\\[1, 2\\], \\[3, 5\\]\\]\n",
    "- b는 1차원 텐서입니다: \\[1, 2\\]\n",
    "\n",
    "**2.  브로드캐스팅 곱셈 :** \n",
    "\n",
    "c = a * b는 텐서 a와 b의 요소별 곱셈을 수행합니다. 이 과정에서 브로드캐스팅이 적용됩니다.\n",
    "\n",
    "**3.  브로드캐스팅 과정 :**\n",
    "\n",
    "- b는 자동으로 \\[\\[1, 2\\], \\[1, 2\\]\\]로 확장됩니다.\n",
    "- 이는 a의 형태와 일치하도록 b가 \"브로드캐스트\" 되는 것입니다.\n",
    "\n",
    "**4.  실제 연산 :** \n",
    "\n",
    "\\[\\[1, 2\\], \\* \\[\\[1, 2\\], = \\[\\[1_1, 2_2\\], \\[3, 5\\]\\] \\[1, 2\\]\\] \\[3_1, 5_2\\]\\]\n",
    "\n",
    "**5.  결과 출력 :**\n",
    "tensor([[ 1,  4],\n",
    "        [ 3, 10]])\n",
    "\n",
    "이 예제는 브로드캐스팅의 강력함을 보여줍니다. 크기가 다른 텐서 간의 연산을 가능하게 하여 코드를 간결하게 만들고 메모리 사용을 최적화합니다. 브로드캐스팅은 딥러닝에서 배치 처리나 다양한 차원의 데이터를 다룰 때 매우 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/2.2_broad.png\" width=\"600\">\n",
    "<figcaption>그림 2.2 브로드캐스팅 곱셈</figcaption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1084,  0.3478,  0.3098,  2.2488, -0.4812],\n",
      "         [ 1.0377, -1.6999,  0.2897,  0.1296,  1.2577],\n",
      "         [-0.3256,  0.2629, -0.2754, -0.3524, -0.5974],\n",
      "         [ 0.0927, -0.8835, -0.1294,  1.4870,  0.7267]],\n",
      "\n",
      "        [[ 0.4977, -0.5008,  0.4119, -0.4718,  0.2138],\n",
      "         [-0.7520, -0.9608, -1.2264,  0.7115,  0.0488],\n",
      "         [ 0.4390, -0.0190,  0.3801, -0.6765,  0.2485],\n",
      "         [-0.5980,  0.5802, -0.5287,  0.2917, -0.3524]],\n",
      "\n",
      "        [[-0.3679,  0.0770, -0.0046,  0.1010, -1.3525],\n",
      "         [ 0.4044, -0.1862,  1.2386, -0.2702, -0.6381],\n",
      "         [-0.2366,  1.6847, -1.5438,  1.0073, -2.8280],\n",
      "         [ 0.8730, -1.0693, -0.2952,  0.0331,  1.2567]]])\n",
      "tensor([[-0.1578, -1.7327,  0.1356, -0.6610,  0.6233],\n",
      "        [-1.1717, -0.4506, -0.6797,  0.0576,  0.7229],\n",
      "        [-0.2202,  1.1627, -0.4552, -0.1684,  0.9227],\n",
      "        [-2.3616, -1.0841, -1.4185, -1.7298,  0.0810]])\n",
      "torch.Size([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 3차원 텐서와 2차원 텐서\n",
    "A = torch.randn(3, 4, 5)  # 크기 (3, 4행, 5열)\n",
    "B = torch.randn(4, 5)     # 크기 (4, 5)\n",
    "\n",
    "C = A + B  # (3, 4, 5) + (4, 5) -> (3, 4, 5)로 브로드캐스팅\n",
    "print(A)\n",
    "print(B)\n",
    "print(C.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 PyTorch를 사용하여 3차원 텐서와 2차원 텐서 간의 브로드캐스팅 연산을 수행하는 예제입니다. 각 부분을 자세히 설명하겠습니다:\n",
    "\n",
    "1.  텐서 생성:\n",
    "    \n",
    "    -   `A = torch.randn(3, 4, 5)`: 3x4x5 크기의 3차원 텐서를 생성합니다. 이는 3개의 4x5 행렬로 구성됩니다.\n",
    "    -   `B = torch.randn(4, 5)`: 4x5 크기의 2차원 텐서(행렬)를 생성합니다.\n",
    "    \n",
    "2.  브로드캐스팅 연산:  \n",
    "    `C = A + B`는 텐서 A와 B의 요소별 덧셈을 수행합니다. 이 과정에서 브로드캐스팅이 적용됩니다.\n",
    "3.  브로드캐스팅 과정:\n",
    "    \n",
    "    -   B(4, 5)는 자동으로 (1, 4, 5)로 확장됩니다.\n",
    "    -   그 다음, B는 A의 첫 번째 차원(3)을 따라 복제되어 (3, 4, 5) 형태로 확장됩니다.\n",
    "    -   이제 A와 확장된 B는 같은 형태(3, 4, 5)를 가지게 되어 요소별 덧셈이 가능해집니다.\n",
    "    \n",
    "4.  결과:\n",
    "    \n",
    "    -   `C.shape`는 (3, 4, 5)를 출력합니다. 이는 결과 텐서 C가 A와 같은 형태를 가짐을 나타냅니다.\n",
    "    \n",
    "\n",
    "이 예제는 PyTorch의 브로드캐스팅 기능을 잘 보여줍니다. 브로드캐스팅은 크기가 다른 텐서 간의 연산을 가능하게 하여 코드를 간결하게 만들고 메모리 사용을 최적화합니다. 이는 특히 배치 처리나 다양한 차원의 데이터를 다룰 때 매우 유용한 기능입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([2, 5])\n",
      "tensor(6)\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 예시 텐서 생성 (2x3 텐서)\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 첫 번째 행만 선택\n",
    "slice_1 = tensor[0, :]\n",
    "# 0은 첫 번째 행을, :은 모든 열을 선택\n",
    "print(slice_1)  # tensor([1, 2, 3])\n",
    "\n",
    "# 두 번째 열만 선택\n",
    "slice_2 = tensor[:, 1]\n",
    "# :은 모든 행을, 1은 두 번째 열(0부터 시작)을 선택\n",
    "print(slice_2)  # tensor([2, 5])\n",
    "\n",
    "# 두 번째 행과 세 번째 열의 원소 선택\n",
    "slice_3 = tensor[1, 2]\n",
    "print(slice_3)  # tensor(6)\n",
    "\n",
    "# 첫 번째 행의 첫 번째와 두 번째 열을 선택, 0:2는 인덱스 0부터 1까지를 의미\n",
    "slice_4 = tensor[0, 0:2]\n",
    "print(slice_4)  # tensor([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 2x2 텐서 두 개 생성\n",
    "tensor_a = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor_b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# 행 기준(0번 축)으로 연결 (2x2 + 2x2 = 4x2)\n",
    "concat_1 = torch.cat((tensor_a, tensor_b), dim=0)\n",
    "print(concat_1)\n",
    "# tensor([[1, 2],\n",
    "#         [3, 4],\n",
    "#         [5, 6],\n",
    "#         [7, 8]])\n",
    "\n",
    "# 열 기준(1번 축)으로 연결 (2x2 + 2x2 = 2x4)\n",
    "concat_2 = torch.cat((tensor_a, tensor_b), dim=1)\n",
    "print(concat_2)\n",
    "# tensor([[1, 2, 5, 6],\n",
    "#         [3, 4, 7, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지 생성된 텐서는 CPU에 저장이 됩니다. \n",
    "- GPU를 사용하고 싶다면 GPU 사용 여부를 확인 하고 .to 메소드를 사용하면 GPU로 텐서를 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA를 사용할 수 없어 텐서가 CPU에 남아있습니다.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch에서 GPU를 사용할 수 있는지 확인하고, 사용 가능한 경우 텐서를 GPU로 이동시키는 예제\n",
    "\n",
    "import torch\n",
    "\n",
    "# 예시 텐서 생성\n",
    "C = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor = C.to(\"cuda\")\n",
    "    print(f\"텐서가 {tensor.device}로 이동되었습니다.\")\n",
    "else:\n",
    "    print(\"CUDA를 사용할 수 없어 텐서가 CPU에 남아있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  `torch.cuda.is_available()`:\n",
    "    \n",
    "    -   이 함수는 시스템에 CUDA 지원 GPU가 있고 PyTorch가 이를 사용할 수 있는지 확인합니다.\n",
    "    -   GPU가 사용 가능하면 True를, 그렇지 않으면 False를 반환합니다.\n",
    "    \n",
    "2.  `if` 조건문:\n",
    "    \n",
    "    -   GPU가 사용 가능한 경우에만 내부 코드를 실행합니다.\n",
    "    \n",
    "3.  `tensor = C.to(\"cuda\")`:\n",
    "    \n",
    "    -   'C'라는 기존 텐서를 GPU 메모리로 이동시킵니다.\n",
    "    -   `.to(\"cuda\")` 메서드는 텐서를 CPU에서 GPU로 전송합니다.\n",
    "    \n",
    "\n",
    "**주의사항 :**\n",
    "\n",
    "-   이 코드는 'C'라는 텐서가 이미 정의되어 있다고 가정합니다.\n",
    "-   GPU로 텐서를 이동시키면 연산 속도가 크게 향상될 수 있습니다.\n",
    "-   GPU에서의 연산은 대규모 신경망이나 데이터셋 처리에 특히 유용합니다.\n",
    "-   GPU가 없는 시스템에서는 텐서가 CPU에 남아 있게 됩니다.\n",
    "\n",
    "이 방식은 GPU가 있는 시스템에서는 자동으로 GPU를 활용하고, 없는 시스템에서도 코드가 정상적으로 실행되도록 하는 유연한 접근법입니다.\n",
    "\n",
    "GPU 가용성에 따라 자동으로 적응하여, 다양한 환경에서 코드의 유연한 실행을 가능하게 합니다. \n",
    "\n",
    "GPU 사용 시 성능이 크게 향상되며, 특히 대규모 신경망이나 데이터셋 처리에 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU 장치 사용 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드는 PyTorch에서 GPU 사용 가능 여부를 확인하고, 사용할 장치를 설정하는 중요한 부분입니다.\n",
    "1.  `torch.cuda.is_available()`: \n",
    "  - torch.cuda.is_available() 함수는 CUDA가 사용 가능한지 확인합니다. \n",
    "  - 이는 시스템에 CUDA 지원 GPU가 있고 PyTorch가 GPU를 사용할 수 있도록 설정되어 있는지 검사합니다.\n",
    "\n",
    "2. `torch.device()` \n",
    "  - torch.device() 함수는 텐서나 모듈을 할당할 장치를 지정합니다.\n",
    "  - 삼항 연산자를 사용하여, CUDA가 사용 가능하면 \"cuda\"를, 그렇지 않으면 \"cpu\"를 선택합니다.\n",
    "  - Pytorch의 Tensor데이터는 GPU ⟺ CPU 으로 연산 위치가 시시각각 변하는 경우가 많기에 선언한 Tensor 변수가 어느 위치에 있어야 하는지 잘 알아야 합니다.\n",
    "결과적으로 device 변수에는 \"cuda\" 또는 \"cpu\" 중 하나가 할당됩니다.\n",
    "\n",
    "\n",
    "이 코드의 장점은 GPU가 있는 시스템에서는 자동으로 GPU를 사용하고, GPU가 없는 시스템에서는 CPU를 사용하도록 하여 코드의 호환성을 높입니다. \n",
    "\n",
    "이후 텐서나 모델을 이 device로 이동시켜 연산을 수행할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
